# SOME DESCRIPTIVE TITLE.
# Copyright (C) The text and illustrations in this website are licensed by the Plone Foundation under a Creative Commons Attribution 4.0 International license.
# This file is distributed under the same license as the Plone 5 Training package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Plone 5 Training 2017\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-03-22 21:00-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../deployment/about/glossary.rst:3
msgid "Glossary"
msgstr ""

#: ../deployment/about/glossary.rst:6
#: ../deployment/ansible.rst:121
msgid "AWS"
msgstr ""

#: ../deployment/about/glossary.rst:8
msgid "`Amazon Web Services <https://aws.amazon.com/>`_ offers reliable, scalable, and inexpensive cloud computing services. ree to join, pay only for what you use."
msgstr ""

#: ../deployment/about/glossary.rst:10
msgid "Linode"
msgstr ""

#: ../deployment/about/glossary.rst:13
msgid "`Linode.com <https://www.linode.com/>`_ is an American privately owned virtual private server provider company"
msgstr ""

#: ../deployment/about/glossary.rst:13
msgid "based in Galloway, New Jersey, United States."
msgstr ""

#: ../deployment/about/glossary.rst:14
msgid "DigitalOcean"
msgstr ""

#: ../deployment/about/glossary.rst:17
msgid "`DigitalOcean, Inc. <https://www.digitalocean.com/>`_ is an American cloud infrastructure provider"
msgstr ""

#: ../deployment/about/glossary.rst:17
msgid "headquartered in New York City with data centers worldwide."
msgstr ""

#: ../deployment/about/glossary.rst:18
msgid "ZODB"
msgstr ""

#: ../deployment/about/glossary.rst:20
msgid "`A native object database for Python <http://www.zodb.org/en/latest/>`_."
msgstr ""

#: ../deployment/about/glossary.rst:21
msgid "TTW"
msgstr ""

#: ../deployment/about/glossary.rst:23
msgid "Through-The-Web, changes in the browser/"
msgstr ""

#: ../deployment/about/glossary.rst:24
msgid "S3"
msgstr ""

#: ../deployment/about/glossary.rst:26
msgid "`Amazon S3 <https://aws.amazon.com/s3/>`_ - Object storage built to store and retrieve any amount of data from anywhere."
msgstr ""

#: ../deployment/about/glossary.rst:27
msgid "NFS"
msgstr ""

#: ../deployment/about/glossary.rst:29
msgid "`Network File System <https://en.wikipedia.org/wiki/Network_File_System>`_."
msgstr ""

#: ../deployment/about/glossary.rst:30
msgid "Amazon OpsWorks"
msgstr ""

#: ../deployment/about/glossary.rst:33
msgid "`AWS OpsWorks <https://aws.amazon.com/opsworks/>`_ is a configuration management service that uses Chef,"
msgstr ""

#: ../deployment/about/glossary.rst:33
msgid "an automation platform that treats server configurations as code."
msgstr ""

#: ../deployment/about/glossary.rst:34
#: ../deployment/intro.rst:54
msgid "Ansible"
msgstr ""

#: ../deployment/about/glossary.rst:37
msgid "`Ansible <https://www.ansible.com/>`_ is an open source automation platform."
msgstr ""

#: ../deployment/about/glossary.rst:37
msgid "Ansible can help you with configuration management, application deployment, task automation."
msgstr ""

#: ../deployment/about/glossary.rst:38
msgid "Chef"
msgstr ""

#: ../deployment/about/glossary.rst:40
msgid "`A configuration management tool written in Ruby and Erlang <https://www.chef.io/chef/>`_."
msgstr ""

#: ../deployment/about/glossary.rst:41
msgid "CloudFormation"
msgstr ""

#: ../deployment/about/glossary.rst:43
msgid "`AWS CloudFormation <https://aws.amazon.com/cloudformation/>`_ gives developers and systems administrators an way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion."
msgstr ""

#: ../deployment/about/glossary.rst:45
msgid "Travis CI"
msgstr ""

#: ../deployment/about/glossary.rst:47
msgid "Travis CI is a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub. Open source projects may be tested at no charge via `travis-ci.org <https://travis-ci.org>`_."
msgstr ""

#: ../deployment/about/glossary.rst:49
msgid "Solr"
msgstr ""

#: ../deployment/about/glossary.rst:51
msgid "`Solr <http://lucene.apache.org/solr/>`_ a the popular, blazing-fast, open source enterprise search platform built on Apache Lucene."
msgstr ""

#: ../deployment/about/glossary.rst:52
msgid "ZCML"
msgstr ""

#: ../deployment/about/glossary.rst:54
msgid "The `Zope Configuration Mark-up Language <https://docs.plone.org/develop/addons/components/zcml.html>`_."
msgstr ""

#: ../deployment/about/index.rst:3
#: ../deployment/index.rst:0
#: ../deployment/maintenance.rst:5
msgid "About"
msgstr ""

#: ../deployment/ansible.rst:3
msgid "Intro To Ansible"
msgstr ""

#: ../deployment/ansible.rst:5
msgid "`Ansible <https://www.ansible.com/>`_ is an open-source configuration management, provisioning and application deployment platform written in Python and using `YAML <http://www.yaml.org/start.html>`_ (YAML Ain't Markup Language) as a configuration language."
msgstr ""

#: ../deployment/ansible.rst:8
msgid "Ansible makes its connections from your computer to the target machine using SSH."
msgstr ""

#: ../deployment/ansible.rst:10
msgid "The one server site requirement is an SSH server. General familiarity with SSH is desirable if you're using Ansible -- as well as being a baseline skill for server administration."
msgstr ""

#: ../deployment/ansible.rst:14
msgid "Installation"
msgstr ""

#: ../deployment/ansible.rst:16
msgid "Ansible is installed on the orchestrating computer -- typically your desktop or laptop. It is a large Python application (though a fraction the size of Plone!) that needs specific Python packages from the Python Package Index (PyPI)."
msgstr ""

#: ../deployment/ansible.rst:19
msgid "That makes Ansible a strong candidate for a Python :program:`virtualenv` installation If you don't have :program:`virtualenv` installed on your computer, do it now."
msgstr ""

#: ../deployment/ansible.rst:22
msgid ":program:`virtualenv` may be installed via an OS package manager, or on a Linux or BSD machine with the command:"
msgstr ""

#: ../deployment/ansible.rst:28
msgid "Once you've got :program:`virtualenv`, use it to create a working directory containing a virtual Python:"
msgstr ""

#: ../deployment/ansible.rst:34
msgid "Then, install Ansible there:"
msgstr ""

#: ../deployment/ansible.rst:41
msgid "Now, to use Ansible, activate that Python environment."
msgstr ""

#: ../deployment/ansible.rst:50
msgid "Trainers: check to make sure everyone understands the basic ``source activate`` mechanism."
msgstr ""

#: ../deployment/ansible.rst:52
msgid "Now, let's get a copy of the *Plone Ansible Playbook*. Make sure you're logged in to your ``ansible_work`` directory."
msgstr ""

#: ../deployment/ansible.rst:55
msgid "Unless you're participating in the development of the playbook, or need a particular fix, you'll want to check out the ``STABLE`` branch."
msgstr ""

#: ../deployment/ansible.rst:57
msgid "The ``STABLE`` branch is a pointer to the last release of the playbook."
msgstr ""

#: ../deployment/ansible.rst:63
msgid "Or,"
msgstr ""

#: ../deployment/ansible.rst:71
msgid "That gives you the Plone Ansible Playbook. You'll also need to install a few Ansible roles. Roles are Ansible playbooks packaged for distribution. You may pick up everything with a single command."
msgstr ""

#: ../deployment/ansible.rst:81
msgid "If you forget that command, it's in the short README.rst file in the playbook."
msgstr ""

#: ../deployment/ansible.rst:85
msgid "The rationale for checking the Plone Ansible Playbook out inside the virtualenv directory is that it ties the two together. Months from now, you'll know that you can use the playbook with the Python and Ansible packages in the virtualenv directory."
msgstr ""

#: ../deployment/ansible.rst:88
msgid "We check out the playbook as a subdirectory of the virtualenv directory so that we can search our playbooks and roles without having to search the whole virtualenv set of packages."
msgstr ""

#: ../deployment/ansible.rst:92
msgid "Ansible Basics"
msgstr ""

#: ../deployment/ansible.rst:95
msgid "Connecting To Remote Machines"
msgstr ""

#: ../deployment/ansible.rst:97
msgid "To use Ansible to provision a remote server, we have two requirements:"
msgstr ""

#: ../deployment/ansible.rst:99
msgid "We must be able to connect to the remote machine using :command:`ssh`; and,"
msgstr ""

#: ../deployment/ansible.rst:101
msgid "We must be able to issue commands on the remote server as root (superuser) via :command:`sudo`."
msgstr ""

#: ../deployment/ansible.rst:103
msgid "You'll need to familiarize yourself with how to fulfill these requirements on the cloud/virtual environment of your choice. Examples:"
msgstr ""

#: ../deployment/ansible.rst:106
msgid "Using Vagrant/VirtualBox"
msgstr ""

#: ../deployment/ansible.rst:108
msgid "You will initially be able to log in as the \"vagrant\" user using a private key that's in a file created by Vagrant. The user \"vagrant\" may issue :command:`sudo` commands with no additional password."
msgstr ""

#: ../deployment/ansible.rst:111
msgid "Using Linode"
msgstr ""

#: ../deployment/ansible.rst:113
msgid "You'll set a root password when you create your new machine. If you're willing to use the root user directly, you will not need a :command:`sudo` password."
msgstr ""

#: ../deployment/ansible.rst:116
msgid "When setting up a Digital Ocean machine"
msgstr ""

#: ../deployment/ansible.rst:118
msgid "New machines are typically created with a root account that contains your ssh public key as an authorized key."
msgstr ""

#: ../deployment/ansible.rst:123
msgid "AWS EC2 instances are typically created with a an account named \"root\" or a short name for the OS, like \"ubuntu\", that contains your ssh public key as an authorized key. Passwordless :command:`sudo` is pre-enabled for that account."
msgstr ""

#: ../deployment/ansible.rst:126
msgid "The most important thing is that you know your setup. Test that knowledge by trying an ssh login and issuing a superuser command."
msgstr ""

#: ../deployment/ansible.rst:137
msgid "Inventories"
msgstr ""

#: ../deployment/ansible.rst:139
msgid "Ansible runs on a local computer, and it acts on one or more remote machines. We tell Ansible how to connect to remote machines by maintaining a text inventory file."
msgstr ""

#: ../deployment/ansible.rst:142
msgid "There is a sample inventory configuration file in your distribution. It's meant for use with a Vagrant-style VirtualBox."
msgstr ""

#: ../deployment/ansible.rst:153
msgid "This inventory file is complicated by the fact that a VirtualBox typically has no DNS host name and uses a non-standard port and a special SSH key file. Because of this we have to specify all those things."
msgstr ""

#: ../deployment/ansible.rst:156
msgid "If we were using a DNS-known hostname and our standard ssh key files, it could be much simpler:"
msgstr ""

#: ../deployment/ansible.rst:162
msgid "Ansible inventory files may list multiple hosts and may have aliases for groups of hosts. See https://docs.ansible.com for details."
msgstr ""

#: ../deployment/ansible.rst:165
#: ../deployment/plone-playbook.rst:34
msgid "Playbooks"
msgstr ""

#: ../deployment/ansible.rst:167
msgid "We're going to cover just enough on Ansible playbooks to allow you to read and customize Plone's playbook. `Ansible's documentation <http://docs.ansible.com>`_ is excellent if you want to learn more."
msgstr ""

#: ../deployment/ansible.rst:170
msgid "In Ansible, an individual instruction for the setup of the remote server is called a _task_. Here's a task that makes sure a directory exists."
msgstr ""

#: ../deployment/ansible.rst:181
msgid "This uses the Ansible ``file`` module to check to see if a directory exists with the designated mode. If it doesn't, it's created."
msgstr ""

#: ../deployment/ansible.rst:184
msgid "Tasks may also have execution conditions expressed in Python syntax and may iterate over simple data structures."
msgstr ""

#: ../deployment/ansible.rst:186
msgid "In addition to tasks, Ansible's basic units are *host* and *variable* specifications."
msgstr ""

#: ../deployment/ansible.rst:188
msgid "An Ansible *playbook* is a specification of tasks that are executed for specified hosts and variables. All of these specifications are in YAML."
msgstr ""

#: ../deployment/ansible.rst:192
msgid "Quick Intro To YAML"
msgstr ""

#: ../deployment/ansible.rst:194
msgid "YAML isn't a markup language, and it isn't a programming language either. It's a data-specification notation like JSON."
msgstr ""

#: ../deployment/ansible.rst:197
msgid "Except that YAML -- very much unlike JSON -- is meant to be written and read by humans. The creators of YAML call it a \"human friendly data serialization standard\"."
msgstr ""

#: ../deployment/ansible.rst:202
msgid "YAML is actually a superset of JSON. Every JSON file is also a valid YAML file."
msgstr ""

#: ../deployment/ansible.rst:205
msgid "But if we fed JSON to the YAML parser, we'd be missing the point of YAML, which is human readability."
msgstr ""

#: ../deployment/ansible.rst:207
msgid "Basic types available in YAML include strings, booleans, floating-point numbers, integers, dates, times and date-times. Structured types are sequences (lists) and mappings (dictionaries)."
msgstr ""

#: ../deployment/ansible.rst:210
msgid "Sequences are indicated by list-member lines with leading dashes:"
msgstr ""

#: ../deployment/ansible.rst:218
msgid "Mappings are indicated with key/value pairs with colons separating keys and values:"
msgstr ""

#: ../deployment/ansible.rst:226
msgid "Complex data structures are designated with indentation:"
msgstr ""

#: ../deployment/ansible.rst:250
msgid "Basic types read as you'd expect:"
msgstr ""

#: ../deployment/ansible.rst:261
msgid "Finally, remember that this is a superset of JSON:"
msgstr ""

#: ../deployment/ansible.rst:268
msgid "Want to turn YAML into Python data structures? Or Python into YAML?"
msgstr ""

#: ../deployment/ansible.rst:271
msgid "Python has several YAML parser/generators. The most commonly used is PyYAML."
msgstr ""

#: ../deployment/ansible.rst:274
msgid "Quick code to read YAML from the standard input and turn it into pretty-printed Python data:"
msgstr ""

#: ../deployment/ansible.rst:280
msgid "Quick Intro To Jinja2"
msgstr ""

#: ../deployment/ansible.rst:282
msgid "YAML doesn't have any built-in way to read a variable. Ansible uses the Jinja2 templating language for this purpose."
msgstr ""

#: ../deployment/ansible.rst:285
msgid "A quick example: Let's say we have a variable ``timezone`` containing the target server's desired timezone setting. We can use that variable in a task via Jinja2's double-brace notation: ``{{ timezone }}``."
msgstr ""

#: ../deployment/ansible.rst:288
msgid "Jinja2 also supports limited Python expression syntax and can read object properties or mapping key/values with a dot notation::"
msgstr ""

#: ../deployment/ansible.rst:293
msgid "There are also various filters and tests available via a pipe notation. For example, we use the ``default`` filter to supply a default value if a variable is undefined."
msgstr ""

#: ../deployment/ansible.rst:307
msgid "Jinja2 also is used as a full templating language whenever we need to treat a text file as a template to fill in variable values or execute loops or branching logic."
msgstr ""

#: ../deployment/ansible.rst:309
msgid "Here's an example from the template used to construct a buildout.cfg:"
msgstr ""

#: ../deployment/ansible.rst:322
msgid "Playbook Structure"
msgstr ""

#: ../deployment/ansible.rst:324
msgid "An Ansible \"play\" is a mapping (or dictionary) with keys for hosts, variables and tasks. A playbook is a sequence of such dictionaries."
msgstr ""

#: ../deployment/ansible.rst:327
msgid "A simple playbook:"
msgstr ""

#: ../deployment/ansible.rst:337
msgid "The value of hosts could be a single host name, the name of a group of hosts, or \"all\"."
msgstr ""

#: ../deployment/ansible.rst:340
msgid "Variables"
msgstr ""

#: ../deployment/ansible.rst:343
msgid "Notifications And Handlers"
msgstr ""

#: ../deployment/ansible.rst:345
msgid "We may also specify \"handlers\" that are run if needed."
msgstr ""

#: ../deployment/ansible.rst:362
msgid "Handlers are run if a matching notification is registered. A particular handler is only run once, even if several notifications for it are registered."
msgstr ""

#: ../deployment/ansible.rst:366
msgid "Roles"
msgstr ""

#: ../deployment/ansible.rst:368
msgid "Ansible has various ways to include the contents of YAML files into your playbook. \"Roles\" do it in a more structured way -- much more like a package."
msgstr ""

#: ../deployment/ansible.rst:371
msgid "Roles contain their own variables, tasks and handlers. They inherit the global variable environment and you may pass particular variables when they are called."
msgstr ""

#: ../deployment/ansible.rst:374
msgid "Plone's Ansible Playbook includes several roles for chores such as setting up the load balancer and web server."
msgstr ""

#: ../deployment/ansible.rst:376
msgid "Other roles are fetched (the role source itself is fetched) by ``ansible-galaxy`` when we use it to set up requirements. Most are fetched from GitHub."
msgstr ""

#: ../deployment/ansible.rst:379
msgid "An simple Ansible playbook using roles:"
msgstr ""

#: ../deployment/ansible.rst:400
msgid "If we want to pass variables to roles, we add their keys and values to the mapping."
msgstr ""

#: ../deployment/ansible.rst:402
msgid "Take a look at the ``when: install_loadbalancer|default(True)`` line above. A ``when`` key in a role or task mapping sets a condition for execution. For conditionals like ``when``, Ansible expects a Jinja2 expression."
msgstr ""

#: ../deployment/ansible.rst:406
msgid "We could also have expressed that ``when`` condition as ``\"{{ install_loadbalancer|default(True) }}\"``. Ansible interprets all literal strings as little Jinja2 templates."
msgstr ""

#: ../deployment/customization.rst:3
msgid "Customized Use"
msgstr ""

#: ../deployment/customization.rst:5
msgid "We intend that you should be able to make most changes by changing default variable settings in your ``local_configure.yml`` file. We've made a serious effort to make sure that all those settings are documented in the `Plone's Ansible Playbook <https://docs.plone.org/external/ansible-playbook/docs/index.html>` documentation."
msgstr ""

#: ../deployment/customization.rst:8
msgid "For example, if you want to change the time at which backup occurs, you can check the doc and discover that we have a `plone-backup-at setting <https://docs.plone.org/external/ansible-playbook/docs/plone.html#plone-backup-at>`_."
msgstr ""

#: ../deployment/customization.rst:11
msgid "The default setting is:"
msgstr ""

#: ../deployment/customization.rst:20
msgid "That's 02:30 every morning."
msgstr ""

#: ../deployment/customization.rst:22
msgid "To make it 03:57 instead, use:"
msgstr ""

#: ../deployment/customization.rst:31
msgid "in your ``local_configure.yml`` file."
msgstr ""

#: ../deployment/customization.rst:34
msgid "Common Customization Points"
msgstr ""

#: ../deployment/customization.rst:36
msgid "Let's review the settings that are commonly changed."
msgstr ""

#: ../deployment/customization.rst:39
msgid "Plone Setup"
msgstr ""

#: ../deployment/customization.rst:42
msgid "Eggs And Versions"
msgstr ""

#: ../deployment/customization.rst:44
msgid "You're likely to want to add Python packages to your Plone installation to enable add-on functionality."
msgstr ""

#: ../deployment/customization.rst:46
msgid "Let's say you want to add `collective.easyform <https://pypi.python.org/pypi/collective.easyform>`_ and `webcouturier.dropdownmenu <https://pypi.python.org/pypi/webcouturier.dropdownmenu>`_."
msgstr ""

#: ../deployment/customization.rst:48
msgid "Add to your ``local_configure.yml``:"
msgstr ""

#: ../deployment/customization.rst:56
msgid "If you add eggs, you should nearly always specify their versions:"
msgstr ""

#: ../deployment/customization.rst:64
msgid "That takes care of packages that are available on the `Python Package Index <https://pypi.python.org/pypi>`_. What if your developing packages via git?"
msgstr ""

#: ../deployment/customization.rst:72
msgid "There's more that you can do with the ``plone_sources`` setting. See the docs!"
msgstr ""

#: ../deployment/customization.rst:76
msgid "Buildout From Git Repository"
msgstr ""

#: ../deployment/customization.rst:78
msgid "It's entirely possible that the buildout created by the playbook won't be adequate to your needs."
msgstr ""

#: ../deployment/customization.rst:80
msgid "If that's the case, you may check out your whole buildout directory via git:"
msgstr ""

#: ../deployment/customization.rst:87
msgid "Make sure you check the `documentation on this setting <https://docs.plone.org/external/ansible-playbook/docs/plone.html#plone-buildout-git-repo>`_."
msgstr ""

#: ../deployment/customization.rst:89
msgid "Even if you use your own buildout, you'll need to make sure that some of the playbook settings reflect your configuration."
msgstr ""

#: ../deployment/customization.rst:92
msgid "Running Buildout And Restarting Clients"
msgstr ""

#: ../deployment/customization.rst:94
msgid "By default, the playbook tries to figure out if :command:`buildout` needs to be run. If you add an egg, for example, the playbook will run buildout to make the buildout-controlled portions of the installation update."
msgstr ""

#: ../deployment/customization.rst:97
msgid "If you don't want that behavior, change it:"
msgstr ""

#: ../deployment/customization.rst:103
msgid "If ``autorun`` is turned off, you'll need to log in to run buildout after it completes the first time. (When you first run the playbook on a new server, buildout will always run.)"
msgstr ""

#: ../deployment/customization.rst:106
msgid "If automatically running buildout bothers you, automatically restarting Plone after running buildout will seem foolish. You may turn it off:"
msgstr ""

#: ../deployment/customization.rst:113
msgid "That gives you the option to log in and run the client restart script. If you're conservative, you'll first try starting and stopping the reserved client."
msgstr ""

#: ../deployment/customization.rst:119
msgid "By the way, if buildout fails, your playbook run will halt. You don't need to worry that an automated restart might occur after a failed buildout."
msgstr ""

#: ../deployment/customization.rst:124
msgid "Web Hosting Options"
msgstr ""

#: ../deployment/customization.rst:126
msgid "It's likely that you're going to need to make some changes in nginx configuration. Most of those changes are made via the ``webserver_virtualhosts`` setting."
msgstr ""

#: ../deployment/customization.rst:129
msgid "``webserver_virtualhosts`` should contain a list of the hostnames you wish to support. For each one of those hostnames, you may make a variety of setup changes."
msgstr ""

#: ../deployment/customization.rst:132
msgid "The playbook automatically creates a separate host file for each host you configure."
msgstr ""

#: ../deployment/customization.rst:134
msgid "Here's the default setting:"
msgstr ""

#: ../deployment/customization.rst:143
msgid "This connects your inventory hostname for the server to the /Plone directory in the ZODB."
msgstr ""

#: ../deployment/customization.rst:145
msgid "A more realistic setting might look something like:"
msgstr ""

#: ../deployment/customization.rst:166
msgid "Here we're setting up two separate hosts, one for http and one for https. Both point to the same ZODB path, though they don't have to."
msgstr ""

#: ../deployment/customization.rst:169
msgid "The https host item also refers to a key/certificate file pair on the Ansible host machine. They'll be copied to the remote server."
msgstr ""

#: ../deployment/customization.rst:172
msgid "Alternatively, you could specify use of certificates already on the server:"
msgstr ""

#: ../deployment/customization.rst:185
msgid "One hazard for the current playbook web server support is that it does **not** delete old host files. If you had previously set up ``www.mynewclient.com`` and then deleted that item from the playbook host list, the nginx host file would remain. Log in and delete it if needed. Yes, this is an exception to the \"don't login to change configuration rule\"."
msgstr ""

#: ../deployment/customization.rst:190
msgid "**Extra tricks**"
msgstr ""

#: ../deployment/customization.rst:192
msgid "There are a couple of extra setting that allow you to do extra customization if you know nginx directives. For example:"
msgstr ""

#: ../deployment/customization.rst:201
msgid "This is a *redirect to https*. It takes advantage of the fact that if you do not specify a zodb_path, the playbook will not automatically create a location stanza with a rewrite and proxy_pass directives."
msgstr ""

#: ../deployment/customization.rst:206
msgid "Mail Relay"
msgstr ""

#: ../deployment/customization.rst:208
msgid "Some cloud server companies do not allow servers to directly send mail to standard mail ports. Instead, they require that you use a *mail relay*."
msgstr ""

#: ../deployment/customization.rst:211
msgid "This is a typical setup:"
msgstr ""

#: ../deployment/customization.rst:221
msgid "Bypassing Components"
msgstr ""

#: ../deployment/customization.rst:223
msgid "Remember our stack diagram? The only part of the stack that you're stuck with is Plone."
msgstr ""

#: ../deployment/customization.rst:226
msgid "All the other components my be replaced. To replace them, first prevent the playbook from installing the default component. Then, use a playbook of your own to install the alternative component."
msgstr ""

#: ../deployment/customization.rst:230
msgid "For example, to install an alternative to the Postfix mail agent, add:"
msgstr ""

#: ../deployment/customization.rst:238
msgid "If you choose not to install the HAProxy, varnish or Nginx, you take on some extra responsibilities. You're going to need to make sure in particular that your port addresses match up. If, for example, you replace HAProxy, you will need to point varnish to the new load-balancer's frontend. You'll need to point the new load balancer to the ZEO clients."
msgstr ""

#: ../deployment/customization.rst:244
msgid "Multiple Plones Per Host"
msgstr ""

#: ../deployment/customization.rst:246
msgid "We've covered the simple case of having one Plone server installed on your server. In fact, you may install additional Plones."
msgstr ""

#: ../deployment/customization.rst:249
msgid "To do so, you create a list variable ``playbook_plones`` containing all the settings that are specific to one or more of your Plone instances."
msgstr ""

#: ../deployment/customization.rst:251
msgid "Nearly all the plone_* variables, and a few others like loadbalancer_port and webserver_virtualhosts may be set in playbook_plones. Here's a simple example:"
msgstr ""

#: ../deployment/customization.rst:274
msgid "Note that you're going to have to specify a minimum of an instance name, a zeo port and a client base port (the address of client1 for this Plone instance.)"
msgstr ""

#: ../deployment/customization.rst:276
msgid "You may specify up to four items in your ``playbook_plones`` list. If you need more, see the docs as you'll need to make a minor change in the main playbook."
msgstr ""

#: ../deployment/customization.rst:280
msgid "The Plone Role -- Using It Independently"
msgstr ""

#: ../deployment/customization.rst:282
msgid "For big changes, you may find that the full playbook is of little or no use. In that case, you may still wish to use Plone's Ansible Role independently, in your own playbooks."
msgstr ""

#: ../deployment/customization.rst:285
msgid "The `Plone server role <https://github.com/plone/ansible.plone_server>`_ is maintained separately, and may become a role in your playbooks if it works for you."
msgstr ""

#: ../deployment/in-operation.rst:3
msgid "In Operation"
msgstr ""

#: ../deployment/in-operation.rst:5
msgid "Hopefully, you've got a provisioned server."
msgstr ""

#: ../deployment/in-operation.rst:7
msgid "Do a quick check by ssh'ing to the server."
msgstr ""

#: ../deployment/in-operation.rst:9
msgid "You should see a welcome message like:"
msgstr ""

#: ../deployment/in-operation.rst:30
msgid "This gives you a list of all the long-lived services installed by the playbook and the interface/ports at which they're attached."
msgstr ""

#: ../deployment/in-operation.rst:32
msgid "Note the service addresses which begin with ``127.0.0.1``. Those services should only answer requests from the server itself: from the localhost. See the firewalling section below for help on tightening this up."
msgstr ""

#: ../deployment/in-operation.rst:36
msgid "How do you connect to local-only ports? Use SSH tunnels."
msgstr ""

#: ../deployment/in-operation.rst:43
msgid "This is a pretty typical login that creates handy tunnels between ports on your local machine with matching HAProxy-admin, varnish and HAProxy frontend ports on the remote server."
msgstr ""

#: ../deployment/in-operation.rst:46
msgid "While you're logged in, check out the status of the :program:`supervisor` process-control system, which is used to launch your Zope/Plone processes."
msgstr ""

#: ../deployment/in-operation.rst:53
msgid "will list all the processes controlled by supervisor."
msgstr ""

#: ../deployment/in-operation.rst:56
msgid "Plone Setup And Directories"
msgstr ""

#: ../deployment/in-operation.rst:58
msgid "While you're logged in, let's take a look at the Plone/Zope setup."
msgstr ""

#: ../deployment/in-operation.rst:60
msgid "You may modify the Zope/Plone directory layout created by the playbook."
msgstr ""

#: ../deployment/in-operation.rst:62
msgid "Unless you do, the Playbook will put Plone's programs and configuration files in ``/usr/local`` by Plone version."
msgstr ""

#: ../deployment/in-operation.rst:64
msgid "Data files will be in ``/var/local``."
msgstr ""

#: ../deployment/in-operation.rst:66
msgid "This split is intended to make it easier to organize backups and to put data on a different physical or logical device."
msgstr ""

#: ../deployment/in-operation.rst:68
msgid "Unless you change it, backups are also under ``/var/local``."
msgstr ""

#: ../deployment/in-operation.rst:70
msgid "It's easy to change this, and it's not a bad idea to have backups on a different device."
msgstr ""

#: ../deployment/in-operation.rst:72
msgid "In terms of file ownership and permissions, the Playbook pretty much follows the practices of the Plone Unified Installer."
msgstr ""

#: ../deployment/in-operation.rst:74
msgid "Program and configuration files are owned by the ``plone_buildout`` user, and data, log and backup files are owned by the ``plone_daemon`` user."
msgstr ""

#: ../deployment/in-operation.rst:76
msgid "A ``plone_group`` is used to give some needed communication, particularly the ability of buildout to create directories in the data space."
msgstr ""

#: ../deployment/in-operation.rst:78
msgid "This means that if you need to run ``bin/buildout`` via login, it must be run as the ``plone_buildout`` user."
msgstr ""

#: ../deployment/in-operation.rst:84
msgid "Typically, you would never start the main ZEO server or its clients directly. That's handled via :program:`supervisorctl`."
msgstr ""

#: ../deployment/in-operation.rst:87
msgid "There's one exception to this rule: the playbook creates a ZEO client named ``client_reserved`` that is not part of the load-balancer pool and is not managed by supervisor."
msgstr ""

#: ../deployment/in-operation.rst:89
msgid "The purpose of this extra client is to allow you to handle run scripts or debug starts without affecting the load-balanced client pool. It's a good idea to use this mechanism to test an updated buildout:"
msgstr ""

#: ../deployment/in-operation.rst:97
msgid "Restart Script"
msgstr ""

#: ../deployment/in-operation.rst:99
msgid "Still logged in? Let's take a look at another part of the install: the restart script."
msgstr ""

#: ../deployment/in-operation.rst:102
msgid "Look in your buildout directory for the scripts directory. In it, you should find ``restart_clients.sh``. (Go ahead and log out if you're still connected.)"
msgstr ""

#: ../deployment/in-operation.rst:106
msgid "This script, which needs to be run as the superuser via :program:`sudo`, is intended to manage hot restarts. Its general strategy is to run through your ZEO clients, sequentially doing the following:"
msgstr ""

#: ../deployment/in-operation.rst:109
msgid "Mark it down for maintenance in HAProxy;"
msgstr ""

#: ../deployment/in-operation.rst:110
msgid "stop client;"
msgstr ""

#: ../deployment/in-operation.rst:111
msgid "start client; wait long enough for it to start listening"
msgstr ""

#: ../deployment/in-operation.rst:112
msgid "Fetch the homepage directly from the client to load the cache. This will be the first request the client receives, since HAProxy hasn't have marked it live yet. When HAProxy marks it live, the cache will be warm."
msgstr ""

#: ../deployment/in-operation.rst:116
msgid "Mark the client available in HAProxy."
msgstr ""

#: ../deployment/in-operation.rst:118
msgid "After running through the clients, it flushes the varnish cache."
msgstr ""

#: ../deployment/in-operation.rst:120
msgid "This is useful if you're running multiple ZEO and using HAProxy for your load balancer."
msgstr ""

#: ../deployment/in-operation.rst:123
msgid "Client Logs"
msgstr ""

#: ../deployment/in-operation.rst:125
msgid "Unless you change it, the playbook sets up the clients to maintain 5 generations of event and access logs. Event logs are rotated at 5MB, access logs at 20MB."
msgstr ""

#: ../deployment/in-operation.rst:129
msgid "Cron Jobs"
msgstr ""

#: ../deployment/in-operation.rst:131
msgid "The playbook automatically creates :command:`cron` jobs for ZODB backup and packing. These jobs are run as ``plone_daemon``."
msgstr ""

#: ../deployment/in-operation.rst:134
msgid "The jobs are run in the early morning in the server's time zone. Backup is run daily; packing weekly."
msgstr ""

#: ../deployment/in-operation.rst:138
msgid "Load Balancing"
msgstr ""

#: ../deployment/in-operation.rst:140
msgid "Let's step up the delivery stack."
msgstr ""

#: ../deployment/in-operation.rst:142
msgid "All but the smallest sample playbooks set up ZEO load balancing via HAProxy. One of the things we gain from HAProxy is good reporting."
msgstr ""

#: ../deployment/in-operation.rst:145
msgid "The web interface for the HAProxy monitor is deliberately not available to a remote connection."
msgstr ""

#: ../deployment/in-operation.rst:147
msgid "It's easy to get around that with an ssh tunnel:"
msgstr ""

#: ../deployment/in-operation.rst:153
msgid "Now we may ask for the web report at ``http://localhost:1080/admin``. Since we're restricting access, we don't bother with a password."
msgstr ""

#: ../deployment/in-operation.rst:159
msgid "Haproxy monitor at http://localhost:1080/admin"
msgstr ""

#: ../deployment/in-operation.rst:161
msgid "If your optimizing, it's a great idea to look at the HAProxy stats to see what kind of queues are building up in your ZEO client cluster."
msgstr ""

#: ../deployment/in-operation.rst:163
msgid "A word about the cluster strategy."
msgstr ""

#: ../deployment/in-operation.rst:165
msgid "We set up our clients with a single ZODB connection thread. There's a trade-off here."
msgstr ""

#: ../deployment/in-operation.rst:168
msgid "Python's threading isn't great on multi-core machines. If you've got only one CPU core available, that's fine."
msgstr ""

#: ../deployment/in-operation.rst:171
msgid "But modern servers typically have several cores; this scheme allows us to keep those cores more busy than they would be otherwise. The cost is somewhat more memory use: a ZEO client with multiple threads does some memory sharing between threads."
msgstr ""

#: ../deployment/in-operation.rst:174
msgid "It's not a lot, but that gives it some memory use advantage over multiple, single-threaded clients. You may want to make that trade off differently."
msgstr ""

#: ../deployment/in-operation.rst:177
msgid "We also have HAProxy set up to only make one connection at a time to each of our ZEO clients. This is also a trade off."
msgstr ""

#: ../deployment/in-operation.rst:180
msgid "We lose the nice client behavior of automatically using different delivery threads for blobs."
msgstr ""

#: ../deployment/in-operation.rst:182
msgid "But, we lower the risk that a request will sit for a long time in an individual client's queue (the client's connection queue, note haproxy's). If someone makes a request that will take several seconds to render and return, we'd like to avoid slowing down the response to other requests."
msgstr ""

#: ../deployment/in-operation.rst:186
msgid "Reverse-proxy Caching"
msgstr ""

#: ../deployment/in-operation.rst:188
msgid "We use Varnish for reverse-proxy caching. The size of the cache and its storage strategy is customizable."
msgstr ""

#: ../deployment/in-operation.rst:191
msgid "By default, we set up 512MB caches. That's probably about right if you're using a CDN (content delivery network), but may be low if if your site is large and you're not using a CDN. The two small samples use Varnish's ``file`` method for cache storage. The larger samples use ``malloc``."
msgstr ""

#: ../deployment/in-operation.rst:196
msgid "Varnish's control channel is limited to use by localhost and has no secret."
msgstr ""

#: ../deployment/in-operation.rst:198
msgid "In a multi-Plone configuration, where you set up multiple, separate Plone servers with separate load-balancing frontends, our VCL setup does the dispatching to the different frontends."
msgstr ""

#: ../deployment/in-operation.rst:202
msgid "Web Hosting"
msgstr ""

#: ../deployment/in-operation.rst:204
msgid "We use Nginx for the outer web server, depending on it to do efficient URL rewriting for virtual hosting and for handling https."
msgstr ""

#: ../deployment/in-operation.rst:206
msgid "We'll have much more to say about virtual hosting later when we talk about how to customize it. What you need to know now is that simple virtual hosting is automatically set up between the hostname you supply in the inventory and the ``/Plone`` site in the ZODB."
msgstr ""

#: ../deployment/in-operation.rst:209
msgid "You should be able to immediately ask for your server via http and get a Plone welcome page."
msgstr ""

#: ../deployment/in-operation.rst:211
msgid "If your inventory hostname does not have a matching DNS host record, you're going to see something like:"
msgstr ""

#: ../deployment/in-operation.rst:216
msgid "Typical virtual hosting error."
msgstr ""

#: ../deployment/in-operation.rst:218
msgid "You're seeing a virtual-hosting setup error. The requested *page* is being returned, but all the resource URLs in the page -- images, stylesheets and JavaScript resources -- are pointing to the hostname supplied in the inventory."
msgstr ""

#: ../deployment/in-operation.rst:221
msgid "You may fix that by supplying a DNS-valid hostname, or by setting up specific virtual hosting. That's detailed below."
msgstr ""

#: ../deployment/in-operation.rst:224
msgid "That's it for the delivery stack. Let's explore the other components installed by the playbook."
msgstr ""

#: ../deployment/in-operation.rst:228
msgid "Postfix"
msgstr ""

#: ../deployment/in-operation.rst:230
msgid "We use Postfix for our mailhost, and we set it up in a send-only configuration. In this configuration, it should not accept connections from the outside world."
msgstr ""

#: ../deployment/in-operation.rst:235
msgid "You will probably have another SMTP agent that's the real mail exchange (MX) for your domain. Make sure that server is configured to accept mail from the ``FROM`` addresses in use on your Plone server."
msgstr ""

#: ../deployment/in-operation.rst:238
msgid "Otherwise, mail exchanges that \"grey list\" may not accept mail from your Plone server."
msgstr ""

#: ../deployment/in-operation.rst:241
msgid "Updating System Packages"
msgstr ""

#: ../deployment/in-operation.rst:243
msgid "On Debian family Linux, the playbook sets up the server for automatic installation of routine updates. We do not set up an automatic reboot for updates that require a system restart."
msgstr ""

#: ../deployment/in-operation.rst:246
msgid "Be aware that you'll need to watch for \"reboot required\" messages and schedule a reboot."
msgstr ""

#: ../deployment/in-operation.rst:249
msgid "Fail2ban"
msgstr ""

#: ../deployment/in-operation.rst:251
msgid "On Debian family Linux, the playbook installs ``fail2ban`` and configures it to temporarily block IP addresses that repeatedly fail login attempts via ssh."
msgstr ""

#: ../deployment/in-operation.rst:254
#: ../deployment/opsworks/maintenance.rst:63
msgid "Monitoring"
msgstr ""

#: ../deployment/in-operation.rst:256
msgid ":program:`logwatch` is installed and configured to email daily log summaries to the administrative email address."
msgstr ""

#: ../deployment/in-operation.rst:259
msgid "Unless you prevent it, :program:`munin-node` is installed and configured to accept connections from the IP address you designate. To make use of it, you'll need to install :program:`munin` on a monitoring machine."
msgstr ""

#: ../deployment/in-operation.rst:262
msgid "The :program:`munin-node` install by the playbook disables many monitors that are unlikely to be useful to a mostly dedicated Plone servers. It also installs a Plone-specific monitor that reports resident memory usage by Plone components."
msgstr ""

#: ../deployment/in-operation.rst:266
msgid "Changes Philosophy"
msgstr ""

#: ../deployment/in-operation.rst:268
msgid "The general philosophy for playbook use is that you make all server configuration changes via Ansible. If you find yourself logging in to change settings, think again. That's the road to having a server that is no longer reproducible."
msgstr ""

#: ../deployment/in-operation.rst:272
msgid "If you've got a significant change to make, try it first on a test server or a Vagrant box."
msgstr ""

#: ../deployment/in-operation.rst:274
msgid "This does not mean that you'll never want to log into the server. It means that you shouldn't do it to change configuration."
msgstr ""

#: ../deployment/index.rst:5
msgid "Automating Plone Deployment"
msgstr ""

#: ../deployment/index.rst:7
msgid "Deploying Plone with Ansible and OpsWorks"
msgstr ""

#: ../deployment/index.rst:0
msgid "Level"
msgstr ""

#: ../deployment/index.rst:8
msgid "All levels"
msgstr ""

#: ../deployment/index.rst:12
#: ../deployment/opsworks/index.rst:9
msgid "This training is meant to be used in a course or read and worked through by an individual user. Instructors should note that this makes it more discursive than it would be if it was only meant for classroom use."
msgstr ""

#: ../deployment/index.rst:15
#: ../deployment/opsworks/index.rst:12
msgid "Many sections may be zipped through in a class, noting to students that the full text is available for review."
msgstr ""

#: ../deployment/index.rst:49
msgid "https://docs.plone.org/manage/deploying/"
msgstr ""

#: ../deployment/intro.rst:3
#: ../deployment/opsworks/intro.rst:3
msgid "Introduction"
msgstr ""

#: ../deployment/intro.rst:5
msgid "The subject of this training is the deployment of Plone for production purposes."
msgstr ""

#: ../deployment/intro.rst:7
msgid "We will, in particular, be focusing on automating deployment using tools which can target a fresh Linux server and create on it an efficient, robust server."
msgstr ""

#: ../deployment/intro.rst:10
msgid "That target server may be a cloud server newly created on :term:`AWS`, :term:`Linode` or :term:`DigitalOcean`."
msgstr ""

#: ../deployment/intro.rst:12
msgid "Or, it may be a virtual machine created for testing on your own desk or laptop."
msgstr ""

#: ../deployment/intro.rst:14
msgid "Our goal is that these deployments be *repeatable*. If we run the automated deployment multiple times against multiple cloud servers, we should get the same results."
msgstr ""

#: ../deployment/intro.rst:17
msgid "If we run the automated deployment against a virtual machine on our laptop, we should be able to test it as if it was a matching cloud server."
msgstr ""

#: ../deployment/intro.rst:19
msgid "The tools we use for this purpose reflect the opinions of the Plone Installer Team. *We are opinionated*."
msgstr ""

#: ../deployment/intro.rst:22
msgid "With a great many years of experience administering servers and Plone, we feel we have a right to our opinions. But, most importantly, we know we have to make choices and support those choices."
msgstr ""

#: ../deployment/intro.rst:25
msgid "The tools we use may not be the ones you would choose."
msgstr ""

#: ../deployment/intro.rst:27
msgid "They may not be the ones we would choose this month if we were starting over."
msgstr ""

#: ../deployment/intro.rst:29
msgid "But, they are tools widely used in the Plone community. They are well-understood, and you should get few \"I've never heard of that\" complaints if you ask questions of the Plone community."
msgstr ""

#: ../deployment/intro.rst:33
msgid "Our Choices"
msgstr ""

#: ../deployment/intro.rst:35
msgid "Linux"
msgstr ""

#: ../deployment/intro.rst:37
msgid "BSD is great. macOS is familiar. Windows works fine, too. But our experience and the majority experience in the Plone community is with Linux for production servers. That doesn't mean you have to use Linux for your laptop or desktop; anything that runs Python is likely fine."
msgstr ""

#: ../deployment/intro.rst:43
msgid "Major distributions"
msgstr ""

#: ../deployment/intro.rst:45
msgid "We're supporting two target distribution families: Debian and EL (RedHat/CentOS). We're going to try to keep this working on the most recent LTS (Long-Term Support release) or its equivalent."
msgstr ""

#: ../deployment/intro.rst:48
msgid "Platform packages"
msgstr ""

#: ../deployment/intro.rst:50
msgid "We use platform packages whenever possible. We want the non-Plone components on your server to be automatically updatable using your platform tools. If a platform package is usable, we'll use it even if it isn't the newest, coolest version."
msgstr ""

#: ../deployment/intro.rst:56
msgid "There are all sorts of great tools for automating deployment. People we respect have chosen Puppet, Salt/Minion and lots of other tools. We chose Ansible because it requires no preinstalled server component, it's written in Python, and its configuration language is YAML, which is awfully easy to read."
msgstr ""

#: ../deployment/intro.rst:61
msgid "And ..."
msgstr ""

#: ../deployment/intro.rst:63
msgid "We'll discuss particular parts of the deployment stack in the next section."
msgstr ""

#: ../deployment/maintenance.rst:3
msgid "Maintenance Strategies"
msgstr ""

#: ../deployment/maintenance.rst:7
msgid "This section covers strategies for long-run maintenance of your playbook."
msgstr ""

#: ../deployment/maintenance.rst:9
msgid "If you're successful with Plone's Ansible Playbook, you will wish to keep an eye on its continued development. You may wish to be able to integrate bug fixes and new features that have become part of the distribution."
msgstr ""

#: ../deployment/maintenance.rst:12
msgid "But, since this project targets production servers, you'll wish to be careful in integrating those changes so that you minimize risk of breaking a live server configuration."
msgstr ""

#: ../deployment/maintenance.rst:16
msgid "Rule 1: If it changes, test it."
msgstr ""

#: ../deployment/maintenance.rst:18
msgid "Using Ansible (or other configuration-management systems) makes it easier to test a whole server configuration. Make use of that fact! You may test by running your playbook against a Vagrant box or against a staging server."
msgstr ""

#: ../deployment/maintenance.rst:22
msgid "Make sure your test server matches the current live configuration. Copy backup Plone data from the live server; restore it on the test server. Then, make your changes in the playbook (or its Ansible support) and run it against the test server. Only on testing success should you run against the live server."
msgstr ""

#: ../deployment/maintenance.rst:28
msgid "Virtualenv"
msgstr ""

#: ../deployment/maintenance.rst:30
msgid "If you followed our installation instructions, you have a Python virtualenv attached to your playbook checkout. That virtualenv has its own installation of Ansible. That's good, because it protects your playbook against unexpected changes in the global environment -- such as Ansible being updated by the OS update mechanisms."
msgstr ""

#: ../deployment/maintenance.rst:34
msgid "You may need or wish to update the installation of Ansible in your virtualenv. If so, make sure you use the copy of :program:`pip` in your virtualenv. Then, test running your playbook with your new Ansible."
msgstr ""

#: ../deployment/maintenance.rst:39
msgid "What Belongs To The playbook And What Doesn't"
msgstr ""

#: ../deployment/maintenance.rst:41
msgid "The general strategy for playbook changes is to not modify anything that's included with the playbook. We've gone to some trouble to make sure that you can make most foreseeable setup changes without touching distribution files."
msgstr ""

#: ../deployment/maintenance.rst:44
msgid "The :file:`local-configure.yml` is an example of this strategy. It is **not** included with the distribution files. It never will be. We will also never include an :file:`inventory.cfg` file."
msgstr ""

#: ../deployment/maintenance.rst:49
msgid "That means that you may safely merge changes from the STABLE branch of https://github.com/plone/ansible-playbook without fear of overwriting those files. You may also create new playbooks; just give them different names. The extra playbooks might handle installs of extra components, firewalling, user setup, whatever."
msgstr ""

#: ../deployment/maintenance.rst:54
msgid "Git Forks"
msgstr ""

#: ../deployment/maintenance.rst:56
msgid "But, what if you want to use version control with your own added files?"
msgstr ""

#: ../deployment/maintenance.rst:58
msgid "In this case, you will wish to *fork* https://github.com/plone/ansible-playbook. Add your extra files to those included with your local checkout of the git fork and push upstream to your git repository."
msgstr ""

#: ../deployment/maintenance.rst:61
msgid "Then, occasionally merge changes from the Plone GitHub account's repository into your fork, typically by rebasing from Plone's upstream repository STABLE branch. Make sure you keep your added files when you do so."
msgstr ""

#: ../deployment/maintenance.rst:65
msgid "Maintenance Strategies -- Multiple Hosts"
msgstr ""

#: ../deployment/maintenance.rst:67
msgid "The :file:`local-configure.yml` file strategy makes it easy to get going with Plone's playbook fast."
msgstr ""

#: ../deployment/maintenance.rst:69
msgid "But it breaks down if you wish to maintain multiple, different hosts with the playbook. Fortunately, there's an easy way to handle the problem."
msgstr ""

#: ../deployment/maintenance.rst:72
msgid "Create a :file:`host_vars` directory inside your playbook directory (the one containing playbook.yml). Now, inside that directory, create one file per target host, each with a name that matches the inventory entry for the host, plus ``.yml``."
msgstr ""

#: ../deployment/maintenance.rst:75
msgid "Each of these files should be the same as the local-configure.yml file that would be used if this was a single host. Delete the no-longer-needed :file:`local-configure.yml` file."
msgstr ""

#: ../deployment/opsworks/index.rst:5
msgid "Plone Deployments With Amazon OpsWorks"
msgstr ""

#: ../deployment/opsworks/installation.rst:3
msgid "Creating Your First Stack"
msgstr ""

#: ../deployment/opsworks/installation.rst:5
msgid "Setting up a Stack with all of its layers is a tedious excersise it :term:`TTW` (Through-The-Web) configuration."
msgstr ""

#: ../deployment/opsworks/installation.rst:7
msgid "Thankfully there's another :term:`AWS` tool called :term:`CloudFormation` that lets us quickly configure a basic Plone stack with the most common layers configured."
msgstr ""

#: ../deployment/opsworks/installation.rst:10
msgid "If you navigate to CloudFormation in the AWS console you'll be presented with the option to create a ``Stack``. Confusingly, a CloudFormation Stack is not the same thing as an OpsWorks Stack, but the former is what we use to automate the creation of the latter so we can use the terms a bit interchangeably."
msgstr ""

#: ../deployment/opsworks/installation.rst:15
msgid "You'll want to download the following file from GitHub: https://raw.githubusercontent.com/alecpm/opsworks-web-python/master/plone_buildout/examples/zeoserver-stack.template"
msgstr ""

#: ../deployment/opsworks/installation.rst:17
msgid "And use the \"Upload a template to Amazon S3\" option to upload the above file which provides a basic ZEO server stack configuration [*]_."
msgstr ""

#: ../deployment/opsworks/installation.rst:20
msgid "You may want to select the EC2 region for you stack before creating the stack, but if you don't you can always clone the stack into another region later. The stack creation will take a few minutes; once it succeeds you can navigate to the OpsWorks control panel to see your new Stack. [*]_"
msgstr ""

#: ../deployment/opsworks/installation.rst:25
msgid "The CloudFormation setup creates a stack outside of a VPC (Virtual Private Cloud), which is probably not ideal since some instance options are not available outside of a VPC."
msgstr ""

#: ../deployment/opsworks/installation.rst:28
msgid "If you want the stack to use a VPC or to be in a different EC2 region than you initially ran CloudFormation from, then you can clone the Stack from the OpsWorks Dashboard to set your desired region and VPC settings."
msgstr ""

#: ../deployment/opsworks/installation.rst:31
msgid "There are a few important settings which CloudFormation is not able to manage and have to be modified after stack creation."
msgstr ""

#: ../deployment/opsworks/installation.rst:33
msgid "The two Apps (``Plone Instances`` and ``Zeoserver``) should be edited to set the ``Data Source`` to ``None`` (this setting is useful for a Relstorage configuration, but does nothing for a ZEO server configuraiton)."
msgstr ""

#: ../deployment/opsworks/installation.rst:37
msgid "You will probably want to use your own buildout repository in these App configurations, but any buildout you use should probably be cloned from the one used in this demo configuration because it provides a number of buildout parts and variables that the deployment recipes expect to be in place:"
msgstr ""

#: ../deployment/opsworks/installation.rst:41
msgid "https://github.com/alecpm/opsworks_example_buildouts"
msgstr ""

#: ../deployment/opsworks/installation.rst:43
msgid "You'll need to provide some additional configuration (Chef Attributes) in the form of the Stack ``Custom JSON`` which can be edited in the Stack Settings control panel."
msgstr ""

#: ../deployment/opsworks/installation.rst:46
msgid "The following should be a reasonable starting point"
msgstr ""

#: ../deployment/opsworks/installation.rst:65
msgid "Note the ``buildout_additional_config`` attribute, which allows you to insert arbitrary configuration and overrides into the generated buildout ``deploy.cfg``."
msgstr ""

#: ../deployment/opsworks/installation.rst:68
msgid "In this case, it's used to set a custom admin password for your new plone instance."
msgstr ""

#: ../deployment/opsworks/installation.rst:70
msgid "There are similar ``buildout_parts_to_include`` and ``buildout_extends`` attributes which allow you to customize the parts used for a particular deploy and any additional configuration files to include."
msgstr ""

#: ../deployment/opsworks/installation.rst:73
msgid "For example, typically I will use a include a ``cfg/production_sources.cfg`` in my production stack which sets revision/tag pins for any external source dependencies in for production deployments."
msgstr ""

#: ../deployment/opsworks/installation.rst:76
msgid "You may also wish to set the ``OpsWorks Agent Version`` to ``Use latest version``, and choose a ``Hostname Theme`` for fun."
msgstr ""

#: ../deployment/opsworks/installation.rst:78
msgid "Note that this default configuration uses a blob directory shared over NFS."
msgstr ""

#: ../deployment/opsworks/installation.rst:80
msgid "That's not necessary if you're going to use a single instance configuration that you plan never to grow (perhaps for a staging server), but if you think you might want multiple servers running ZEO clients, then starting out with a network shared storage for blobs is probably the best way to go."
msgstr ""

#: ../deployment/opsworks/installation.rst:84
msgid "You could also configure shared blobs using the GlusterFS distributed filesystem (this can be tricky and is only recommended if you are already familiar with GlusterFS), S3-fuse Fs (slow), or serve them from the ZEO Server or RelstorageDB."
msgstr ""

#: ../deployment/opsworks/installation.rst:88
msgid "If you do want a single server configuration with no network blob share, then you will need to add a line of configuration for the blob storage location, e.g.::"
msgstr ""

#: ../deployment/opsworks/installation.rst:95
msgid "You may also want to change the load balancer stats access password in the HAProxy layer."
msgstr ""

#: ../deployment/opsworks/installation.rst:97
msgid "By default, each server is protected by a firewall that only allows access to specific services defined by that instances layers."
msgstr ""

#: ../deployment/opsworks/installation.rst:99
msgid "Our layers are heavily customized, so the defaults aren't always sufficient."
msgstr ""

#: ../deployment/opsworks/installation.rst:101
msgid "You'll want to ensure that the servers can all communicate with one another over all desired ports, and you'll probably want to be able to bypass the default firewall from specific external IP addresses to get direct access to your ZEO Clients, etc."
msgstr ""

#: ../deployment/opsworks/installation.rst:105
msgid "The simplest way to do that is to go to the Security tab for each of the Layers and add the ``default`` security group to each of them. [*]_"
msgstr ""

#: ../deployment/opsworks/installation.rst:108
msgid "There is also a RelStorage version of this template, though turning a Zeoserver Stack into a Relstorage Stack simply involves deleting the ZEO server Layer and adding a built-in Memcached Layer."
msgstr ""

#: ../deployment/opsworks/installation.rst:110
msgid "Before creating a CloudFormation Stack you'll be asked to confirm that AWS resources may be created. The stack template here only creates cost-free configuration resources."
msgstr ""

#: ../deployment/opsworks/installation.rst:112
msgid "This could be done with more granularity, but ``default`` is usually a safe bet. By default, ``default`` allows servers within your VPC full access to one another, but doesn't permit any outside access. You can configure the ``default`` security group to allow your personal IPs direct access to any specific ports you may want want to access remotely."
msgstr ""

#: ../deployment/opsworks/installation.rst:116
msgid "Adding An Instance"
msgstr ""

#: ../deployment/opsworks/installation.rst:122
msgid "At this point you can navigate to the ``Instances`` control panel and create an instance in a particular layer. Once you've defined your first instance you can assign it to additional layers."
msgstr ""

#: ../deployment/opsworks/installation.rst:125
msgid "Once you pick an appropriate instance size (t2.micro is fine for playing around), you should be able to use the instance defaults, though the initial EBS volume size is something you may want to configure later if you don't intend to use separate mount points for data storage."
msgstr ""

#: ../deployment/opsworks/installation.rst:129
msgid "Once you've created the first instance you'll want to add it other layers using the ``Existing OpsWorks`` tabs. You will probably want to skip the ``EBS Snapshotting`` layer for now, and if you disabled NFS you should skip the ``Shared Blobs`` layer too."
msgstr ""

#: ../deployment/opsworks/installation.rst:132
msgid "By default the ``Zeoserver`` layer and the ``Shared Blobs`` layer both create and attach EBS volumes to any instances assigned to them (for the filestorage and NFS shared blobstorage respectively)."
msgstr ""

#: ../deployment/opsworks/installation.rst:135
msgid "This is optional when using an EBS backed instance with an adequately sized root volume, but is mandatory when using instance store backed instances."
msgstr ""

#: ../deployment/opsworks/installation.rst:137
msgid "Traditionally, instance store backed instances had some performance and cost advantages, but those advantages have largely vanished recently, and EBS instances can stop and start much faster after initial instance creation."
msgstr ""

#: ../deployment/opsworks/installation.rst:140
msgid "For testing you may want to delete the EBS volume resources from those layers before starting your instance."
msgstr ""

#: ../deployment/opsworks/installation.rst:144
msgid "I still like using instance store instances with sepearate attached EBS volumes because those instances make no promises about retaining configuration changes outside of the explicitly mounted EBS volumes, and that keeps me from twiddling server configuration in ways that might not be repeatable."
msgstr ""

#: ../deployment/opsworks/installation.rst:150
msgid "They also help avoid some I/O concurrency issues you may run into with an all EBS configuration, and allow more straighforward vertical scaling."
msgstr ""

#: ../deployment/opsworks/installation.rst:154
msgid "Now you should be able to start your instance, and after a little while (depending on the instance size), you will have a server up and running."
msgstr ""

#: ../deployment/opsworks/installation.rst:156
msgid "This Zope instance won't have a Plone site yet, so having added the ``default`` security group earlier to allow yourself direct access to the ZEO clients will come in handy here."
msgstr ""

#: ../deployment/opsworks/installation.rst:159
msgid "Your instance should have a public IP address (the frontend layer assigns an Elastic IP by default, though you could manually transfer one in if you were moving an existing EC2 server)."
msgstr ""

#: ../deployment/opsworks/installation.rst:162
msgid "You should be able to access the first ZEO client at port 8081 and create your Plone site."
msgstr ""

#: ../deployment/opsworks/installation.rst:166
msgid "Caveats"
msgstr ""

#: ../deployment/opsworks/installation.rst:168
msgid "There are a few restrictions on what can and can't be done when in of OpsWorks which can occasionally cause annoyance:"
msgstr ""

#: ../deployment/opsworks/installation.rst:170
msgid "Instances can only be added to layers when the Instances are stopped,you cannot add additional Layers of functionality to an already running Instance. There are workarounds for this limitation (such as adding recipes or package dependencies to existing layers and re-running the relevant phases), but it can be frustrating."
msgstr ""

#: ../deployment/opsworks/installation.rst:174
msgid "You cannot change the security groups of a running instance, and changes to a Layer's security groups don't apply to running instances. Thankfully, any changes to the firewall rules for a security group will affect all running instances in that group. It's best to make sure your Layers assign all the security groups you might need before starting an instance."
msgstr ""

#: ../deployment/opsworks/installation.rst:177
msgid "A setup or deploy may fail because of problems accessing Repositories or PyPI packages. If the initial instance setup fails, it is not generally necessary to stop, wait and then start the instance (which can take a long time), you generally can re-run the ``setup`` phase from the Stack panel using the ``Run Command`` button."
msgstr ""

#: ../deployment/opsworks/installation.rst:181
msgid "Downloading public packages from PyPI and dist.plone.org is often the slowest part of initial instance setup. It can help tremendously to have a tarball of all required eggs stored in a public S3 url, you can use the Custom JSON to tell OpsWorks to fetch this tarball before running the buildout. The configuration goes under the ``deploy[app_name]`` key and looks like [*]_ ::"
msgstr ""

#: ../deployment/opsworks/installation.rst:188
msgid "This configuration assumes that the tarball has top-level directory called ``eggs``. If you've setup such a tarball in an S3 bucket (usually creating it from your first instance deploy), you simply add this configuration to both the ``deploy[\"plone_instances\"]`` and ``deploy[\"zeoserver\"]`` Custom JSON before launching an instance."
msgstr ""

#: ../deployment/opsworks/intro.rst:5
msgid "The subject of this training is using the :term:`Amazon OpsWorks` deployment system to orchestrate complex, scalable, and redundant multi-server deployments of Plone."
msgstr ""

#: ../deployment/opsworks/intro.rst:8
msgid "The tools presented herein provide a mechanism for generically defining server requirements and resources to launch fully configured Amazon EC2 instances running Plone in a coordinated distributed manner."
msgstr ""

#: ../deployment/opsworks/intro.rst:11
msgid "Amazon OpsWorks does not provide the flexibility of :term:`Ansible` deployments. It is tied to Amazon cloud infrastructure, and is only fully tested for servers running Ubuntu LTS."
msgstr ""

#: ../deployment/opsworks/intro.rst:14
msgid "It does provide an unique infrastructure to automate communication among multiple servers, allowing automated discovery and inclusion of resources, and facilitating features like auto-scaling and auto- healing."
msgstr ""

#: ../deployment/opsworks/intro.rst:17
msgid "OpsWorks is built on :term:`Chef`, which is a configuration management system similar to Ansible, but built on Ruby [*]_."
msgstr ""

#: ../deployment/opsworks/intro.rst:19
msgid "The tools and concepts described here attempt to ensure that you can deploy a complex Plone site without having to learn any Chef or Ruby."
msgstr ""

#: ../deployment/opsworks/intro.rst:22
msgid "Yuck!"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:3
msgid "What Doesn't It Do"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:6
msgid "Storage Options"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:8
msgid "Amazon introduced ``Elastic File System`` an effectively unlimited size cloud file storage that can be mounted simultaneously on multiple servers."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:11
msgid "It provides high availability and durability and should be significantly faster than either :term:`S3` or even standard SSD EBS mounts."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:13
msgid "For these reasons it would make an ideal storage option for a shared blob directory and possibly also ZEO filestorages."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:15
msgid "Integrating this new storage option into the recipes and documentation should be a high priority going forward. The interface for Elastic File System is :term:`NFS` v4, which the stack already supports, so it may even be trivial to integrate."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:18
msgid "There are probably some other fun new :term:`AWS` services that would be useful to integrate."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:22
msgid "Proxy Cache Purging"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:24
msgid "Plone provides some very nice proxy caching configuration, but that configuration is managed :term:`TTW` (Through-The-Web) and stored persistently in the :term:`ZODB`."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:27
msgid "If you have multiple proxy caches which could be going online or offline automatically or changing IP addresses, then having persistent configuration of caches to purge is not ideal."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:30
msgid "It would be very useful to add support in `plone.app.caching <https://github.com/plone/plone.app.caching>`_ for reading a list of proxy servers from an environment variable or other mechanism that can be managed as part of the configuration phase."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:35
msgid "Chef 12"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:37
msgid "The latest OpsWorks codebase requires Chef 12. The Python cookbooks are currently only tested on Chef 11."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:40
msgid "Running OpswWrks on Ubuntu Xenial instances requires using the latest Chef 12 version."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:44
msgid "This will likely require extensive testing and upgrades to dependency cookbooks."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:3
msgid "Maintenance"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:6
msgid "Backups"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:8
msgid "The recipes automatically setup weekly ZODB packing and log rotation. I like to use Amazon's EBS snapshot feature for backups."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:11
msgid "The EBS Snapshotting layer provides that functionality automatically."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:13
msgid "It requires you to use the AWS IAM Console to create a new user with the following permissions::"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:21
msgid "You will need to note the API credentials for this new user and enter them into the Stack"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:23
msgid "Custom JSON as follows:"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:34
msgid "The EBS Snapshotting Layer should be assigned to any production instance which has EBS volumes on which you are storing data. Generally speaking, any production instance with the Zeoserver, Shared Blob, or Solr Layers assigned should also have the EBS Snapshotting Layer assigned."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:38
msgid "This Layer will setup automatic nightly snapshots of all mounted EBS volumes. By default it retains up to 15 snapshots, but that can be configured setting ``ebs_snapshots[\"keep\"]`` to the number you wish to retain in the Stack Custom JSON."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:45
msgid "Updates"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:47
msgid "Ubuntu security and OS package updates can be automated by adding the following Custom JSON configuration:"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:65
msgid ":term:`AWS` provides various monitoring and alerting features, but most alerting features need to be manually configured on a per EC2 instance basis."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:67
msgid "That's not so convenient for a stack of instances which may grow, shrink or change over time."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:69
msgid "For that reason I like to use New Relic for server monitoring."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:71
msgid "There is built-in integration in the recipes, which includes detailed performance server and client performance monitoring for Plone, as well as plugins for Nginx, Varnish and HAProxy services and standard CPU, Disk space and RAM server metrics."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:74
msgid "There's also a recipe provided to integrate the `Papertrail <https://papertrailapp.com/>`_ log tracing and searching service."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:76
msgid "To help you live the dream of never having to SSH into your servers."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:80
msgid "Sending Mail"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:82
msgid "It's possible, and not difficult to install and configure a mailer using a chef postfix recipe and some more Custom JSON."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:84
msgid "I do not recommend doing so."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:86
msgid "Cloud Servers generally, and EC2 specifically tend to land on SPAM blacklists, ensuring your outgoing mail is not blackholed generally requires some special care and requests to Amazon to setup reverse DNS and whitelist any outgoing mail servers."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:89
msgid "Instead I recommend using a hosted mail delivery service like Amazon SES or perhaps GMail."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:93
msgid "SSH Access"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:95
msgid "Ideally, you never have to login to your cloud server, but things go wrong and you might have to eventually, even if only out of curiosity."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:97
msgid "By default OpsWorks does not assign an SSH key to new instances, but you can set one if desired at either the instance or the Stack level."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:99
msgid "Better yet, OpsWorks allows more granular access control in combination with IAM."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:101
msgid "If you create a user via the AWS IAM console (no permissions need be assigned, and no credentials added or recorded for SSH access), you can then import that user into the OpsWorks Users control panel."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:104
msgid "In OpsWorks users can be given access to specific stacks, allowing them to view, deploy or manage them, as well as granting them SSH and/or :command:`sudo` access to Stack Instances using a public key that can be added through the web interface."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:107
msgid "Once you've imported an IAM user into OpsWorks and granted it SSH access with a public key, that user should be able to log in to all instances in the stack. [*]_"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:112
msgid "A note on OS permissions: all application related files live under ``/srv/www`` and are generally owned by the ``deploy`` user with fairly restricted permissions. Any user SSH'ing in will probably need to :command:`sudo` to the ``deploy`` user to see or do much of interest."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:117
msgid "You should *never* manually modify any configuration on a cloud configured server, except for purposes of troubleshooting. Any changes you make to the server should be made via the Stack configuration (i.e. the Custom JSON and the Recipes assigned to Layers)."
msgstr ""

#: ../deployment/opsworks/running.rst:3
msgid "Deploying Changes"
msgstr ""

#: ../deployment/opsworks/running.rst:5
msgid "Now that you've got one or more Instances up and running, you may need to update the code on them. Traditionally, you'd SSH into the server pull in new changes from the repositories, run buildout, and restart ZEO clients if necessary."
msgstr ""

#: ../deployment/opsworks/running.rst:8
msgid "With OpsWorks, you click a deploy button and everything is handled automtically."
msgstr ""

#: ../deployment/opsworks/running.rst:10
msgid "What happens when you click the Deploy button for an OpsWorks App?"
msgstr ""

#: ../deployment/opsworks/running.rst:12
msgid "The instance looks to see if there's a new revision on the App's buildout repository (accounting for the branch or revision setting in the App configuration)."
msgstr ""

#: ../deployment/opsworks/running.rst:14
msgid "If there are changes to the repository, then it makes a new clone of the repository and puts it in a directory under ``releases`` named by the checkout timestamp. It then generates a new ``deploy.cfg`` based on the Stack Configuration, including information about currently running instances and layers. Then it runs bootstrap and buildout with that configuration. If the process succeeds, it symlinks the ``release/$timestamp`` directory to ``current`` and restarts the ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:16
msgid "If there are no changes to the repository, then it generates a new ``deploy.cfg`` based on the current Stack configuration. If that file differs from the existing deploy.cfg (e.g. because of changes in the Stack's Custom JSON), then it will re-run buildout, and - if the buildout succeeds - restart the ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:20
msgid "If there are no changes to the repository, and the new ``deploy.cfg`` is identical to the prior version, then it checks for an ``always_buildout_on_deploy`` flag in the Stack's ``deploy[appname]`` Custom JSON. If that flag is true, then it runs buildout and restarts the ZEO clients on success. This is useful if you are deploying changes from external repositories pulled in by mr.developer, even when the buildout repository itself hasn't changed."
msgstr ""

#: ../deployment/opsworks/running.rst:25
msgid "Steps 1 and 2 are essentially a ``Capistrano`` style deployment familiar from the Rails world. This process allows for explicit rollback of deployed code to prior versions at any time."
msgstr ""

#: ../deployment/opsworks/running.rst:28
msgid "Steps 3 and 4 are buildout specific and don't support rollbacks in the same way."
msgstr ""

#: ../deployment/opsworks/running.rst:30
msgid "You can run a deploy on a single Instance or on many at once. The deploy will run in parallel on all Instances selected."
msgstr ""

#: ../deployment/opsworks/running.rst:33
msgid "Instances that have the deployed App/Layer assigned will go through the process above, other instances will run a generic deploy phase (which allows those Instances to update their configuration in parallel)."
msgstr ""

#: ../deployment/opsworks/running.rst:36
msgid "This process creates a good chance that all your ZEO clients will be restarted at once, causing a temporary outage and a slow site."
msgstr ""

#: ../deployment/opsworks/running.rst:38
msgid "If you have multiple Instances running ZEO clients you can deploy to them one at a time, to avoid an outage."
msgstr ""

#: ../deployment/opsworks/running.rst:40
msgid "You can also configure your Stack to do rolling deploys by adding a ``restart_delay`` in seconds to your Custom JSON under the top-level ``plone_instances`` key."
msgstr ""

#: ../deployment/opsworks/running.rst:43
msgid "When that is set, the deploy will wait that amount of time between each ZEO client restart for a given Instance."
msgstr ""

#: ../deployment/opsworks/running.rst:45
msgid "If you use :term:`Travis CI` to provide automatic testing of your buildout/application, you can configure Travis to automatically launch an OpsWorks deploy for a specific Stack and Application on successful builds (see https://docs.travis-ci.com/user/deployment/opsworks/)."
msgstr ""

#: ../deployment/opsworks/running.rst:50
msgid "Instance Sizes"
msgstr ""

#: ../deployment/opsworks/running.rst:52
msgid "There are many available Instance types on EC2, which makes choosing the correct Instance sizes for your application cluster a bit of an art."
msgstr ""

#: ../deployment/opsworks/running.rst:54
msgid "The OpsWorks recipes will automatically factor in the CPU capacity of the Instances you choose for your ZEO client Layers (using their Elastic Compute Unit/Core counts) to determine automatically how many ZEO clients to create per Instance."
msgstr ""

#: ../deployment/opsworks/running.rst:57
msgid "You can fine tune that calculation further by setting the ``per_cpu`` attribute under the ``plone_instances`` key in the Stack Custom JSON."
msgstr ""

#: ../deployment/opsworks/running.rst:59
msgid "You can also tweak the ``zodb_cache_size``, and ``zserver_threads`` to help tune RAM usage for your ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:63
msgid "Scaling"
msgstr ""

#: ../deployment/opsworks/running.rst:65
msgid "If you've setup a distributed blob storage (whether with NFS/GlusterFS, S3FS, ZEO or Relstorage), adding more ZEO clients is a simple matter of defining a new instance assigned only to your Plone Instances application Layer and starting it."
msgstr ""

#: ../deployment/opsworks/running.rst:68
msgid "In addition to the normal 24/7 instances, you can define time-based Instances that automatically add instances during regular peak traffic periods."
msgstr ""

#: ../deployment/opsworks/running.rst:73
msgid "Alternatively, you can define load-based instances which automatically start up and shutdown based on the average CPU usage, Load, or RAM usage of existing instances in the layer."
msgstr ""

#: ../deployment/opsworks/running.rst:79
msgid "Any new instances will automatically discover your existing ZEO server. Any load balancers will automatically discover any new ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:82
msgid "The Stack reconfiguration will happen automatically whenever an instance goes up or down."
msgstr ""

#: ../deployment/opsworks/running.rst:84
msgid "You can view the HAProxy ZEO client status by visiting the password protected url ``/balancer/stats`` for your frontend instance IP(s)."
msgstr ""

#: ../deployment/opsworks/running.rst:86
msgid "For a high traffic site that requires a high availability configuration, it may also make sense to run the frontend HAProxy layer on multiple Instances in different Availablilty Zones."
msgstr ""

#: ../deployment/opsworks/running.rst:89
msgid "You would need to route external traffic to those servers using an adaptive DNS service or Amazon's Elastic Load Balancer."
msgstr ""

#: ../deployment/opsworks/running.rst:93
msgid "Configuration"
msgstr ""

#: ../deployment/opsworks/running.rst:95
msgid "The Stack Custom JSON configuration offers a number of entry points for customizing the default Stack without needing to learn any Chef or Ruby."
msgstr ""

#: ../deployment/opsworks/running.rst:97
msgid "Those configuration parameters are thoroughly documented in the Plone Buildout cookbook `README`_, and the full list of Plone Buildout cookbook specific attributes is in `attributes/default.rb`_."
msgstr ""

#: ../deployment/opsworks/running.rst:100
msgid "Any of those attributes can be customized via the Stack Custom JSON. For example, the ``nginx_plone[\"additional_configuration\"]`` and ``nginx_plone[\"additional_servers\"]`` may be the most generically useful items for frontend configuration."
msgstr ""

#: ../deployment/opsworks/running.rst:104
msgid "The recipes and example buildout also include optional support for running and configuring a Solr search server and setting up a Celery task queue for running asynchronous jobs using collective.celery."
msgstr ""

#: ../deployment/opsworks/terms.rst:3
msgid "Deployment Terminology"
msgstr ""

#: ../deployment/opsworks/terms.rst:5
msgid "It's probably a good idea to be familar with a few core Chef concepts, though digging deeply into Chef is definitely not something I encourage Python developers to do."
msgstr ""

#: ../deployment/opsworks/terms.rst:10
msgid "``Resource``: The basic building block in Chef (and also Ansible); defines files, directories, installed packaes, services, etc."
msgstr ""

#: ../deployment/opsworks/terms.rst:12
msgid "``Recipe``: A collection of resource definitions with some logic to connect them. These can be very simple or extraordinarily complex; A recipe can depend on other recipes. These basically play the same role as ``Tasks`` in Ansible."
msgstr ""

#: ../deployment/opsworks/terms.rst:14
msgid "``Cookbook``: A collection of recipes required to setup a service or similar. These play a similar role to ``Roles`` in Ansible. These generally can be found in the Chef Supermarket like Roles from the Ansible Galaxy."
msgstr ""

#: ../deployment/opsworks/terms.rst:16
msgid "``Berkshelf``: A single file configuration defining the set of cookbooks needed for a deployment. It consists of a ``Berksfile`` which defines locations and versions of all cookbooks required for a deployment."
msgstr ""

#: ../deployment/opsworks/terms.rst:18
msgid "``Attributes``: The deployment specific configuration for the cookbook and recipes. This is essentially a collection of JSON like primitives, similar to YAML group/host ``Vars``."
msgstr ""

#: ../deployment/opsworks/terms.rst:22
msgid "Opsworks"
msgstr ""

#: ../deployment/opsworks/terms.rst:24
msgid "Amazon OpsWorks takes this basic configuration framework and provides its own set of concepts, to implement cluster orchestration."
msgstr ""

#: ../deployment/opsworks/terms.rst:26
msgid "When using OpsWorks, you will be making use built-in OpsWorks Chef Cookbooks provided by Amazon. These built-in Cookbooks provide a number of Recipes for configuring and deploying many types of applications using TTW (Through-The-Web) configuration from the OpsWorks control panel."
msgstr ""

#: ../deployment/opsworks/terms.rst:30
msgid "These include `Node.js <https://nodejs.org/en/>`_, `Rails <http://rubyonrails.org/>`_, `PHP <https://secure.php.net/>`_ , and Java applications, but not `Python <https://www.python.org/>`_ [*]_."
msgstr ""

#: ../deployment/opsworks/terms.rst:32
msgid "I've created a couple supplemental Cookbooks that extend the existing OpsWorks deployment recipes to support Python and Plone along with other supporting services that are useful when making production deployments of Plone."
msgstr ""

#: ../deployment/opsworks/terms.rst:36
msgid "OpsWorks has its own vocabulary of concepts related to deploying and orchestrating clusters of servers."
msgstr ""

#: ../deployment/opsworks/terms.rst:38
msgid "The building blocks of OpsWorks are:"
msgstr ""

#: ../deployment/opsworks/terms.rst:40
msgid "``Stack``: The fundamental container for your configuration, this lives in a particular EC2 region and contains all the configuration for your cluster. Typically you would have a separate production stack and development stack. Creating this is the first step in the process of defining your cluster. Stacks can be cloned to replicate configuration across regions."
msgstr ""

#: ../deployment/opsworks/terms.rst:45
msgid "``Layers``: A Layer defines a discrete set of functionality that may be provided by a server Instance. For example, a Plone cluster may have a frontend Layer running an Nginx web server, Varnish proxy cache and HAProxy load balancer [*]_ , an Application Layer for your ZEO client instances, an Application Layer for your ZEO server, and a maintenance layer to manage database backups and packs. Layers define what recipes will be run on an instance, and which OS packages it requires, along with any Amazon resources and permissions are required to provide a service (e.g. static IP addresses, additional EBS storage volumes)."
msgstr ""

#: ../deployment/opsworks/terms.rst:51
msgid "``Instances``: An OpsWorks Instance is similar to an EC2 instance, it has a type (e.g. from micro to xlarge), an OS and an Availability Zone, but it is an abstraction. It becomes an actual EC2 instance once it's been started, but before that it's simply a metadata about a desired server. Instances are assigned to one or more Layers, and come in three varieties, 24/7, time-based and load-based."
msgstr ""

#: ../deployment/opsworks/terms.rst:55
msgid "``Apps``: An App points to a code repository (in our case a buildout) which you want to deploy to a specific Application Layer. Typically you would have an App for your Plone instances and another for your Zeoserver. Both these Apps would typically point to the same buildout repository. You might also create an App to configure a Plone specific Solr server or to run a additional applications within the cluster."
msgstr ""

#: ../deployment/opsworks/terms.rst:57
msgid "``Resources``: A set of Amazon EC2 resources that will be used by the stack by being attached/assigned to Instances when they are started. These include Elastic IP addresses, EBS storage volumes and RDS databases (useful you are running Relstorage)."
msgstr ""

#: ../deployment/opsworks/terms.rst:60
msgid "A Stack can be configured with a single Instance running all the Layers, or multiple Instances each running different Layers. You might, for example, have a production stack with five Instances running the Plone ZEO client Application Layer, a single instance running the ZEO server Application Layer, and two Instances running the front end proxy/loadbalancer Layer (with an Elastic Load Balancer in front of those)."
msgstr ""

#: ../deployment/opsworks/terms.rst:65
msgid "You might also have a staging stack with all the same Layers applied to a single modest server. Other than the Instance definitions (and perhaps the App repository branch), these Stacks would be essentially identical."
msgstr ""

#: ../deployment/opsworks/terms.rst:68
msgid "Boo!"
msgstr ""

#: ../deployment/opsworks/terms.rst:69
msgid "Though you could separate each of these frontend services into their own layers if you wanted to, we combine them by default under a customized HAProxy layer which already provides a nice UI for a few HAProxy features."
msgstr ""

#: ../deployment/opsworks/terms.rst:73
msgid "Instance Lifecycle"
msgstr ""

#: ../deployment/opsworks/terms.rst:75
msgid "Each OpsWorks Instance goes through a few phases during its lifecycle:"
msgstr ""

#: ../deployment/opsworks/terms.rst:77
msgid "``setup``"
msgstr ""

#: ../deployment/opsworks/terms.rst:78
msgid "``deploy``"
msgstr ""

#: ../deployment/opsworks/terms.rst:79
msgid "``configure``"
msgstr ""

#: ../deployment/opsworks/terms.rst:80
msgid "``undeploy``"
msgstr ""

#: ../deployment/opsworks/terms.rst:81
msgid "``shutdown``"
msgstr ""

#: ../deployment/opsworks/terms.rst:83
msgid "Each of these lifecycle phases runs recipes assigned to that phase in the assigned Layers. When these recipes are run, the Stack configuration is passed to the server."
msgstr ""

#: ../deployment/opsworks/terms.rst:86
msgid "This configuration includes complete information about the state of the entire Stack and all of its running Instances."
msgstr ""

#: ../deployment/opsworks/terms.rst:88
msgid "When an Instance starts, it first goes through a ``setup`` phase: installing all package dependencies for all assigned Layers and running all the recipes assigned to the ``setup`` phase of those Layers."
msgstr ""

#: ../deployment/opsworks/terms.rst:91
msgid "Once the ``setup`` phase is complete, a ``deploy`` phase is automatically started. Running all the recipes assigned to the ``deploy`` phase of any assigned Layers."
msgstr ""

#: ../deployment/opsworks/terms.rst:94
msgid "Subsequently, you may manually run a ``deploy`` for a specific Application on any or all of the instances to update the application code and reconfigure services."
msgstr ""

#: ../deployment/opsworks/terms.rst:97
msgid "The ``shutdown`` phase is run automatically before an instance is stopped."
msgstr ""

#: ../deployment/opsworks/terms.rst:99
msgid "The ``undeploy`` phase is rarely used. It is triggered when an application is manually removed from an instance."
msgstr ""

#: ../deployment/opsworks/terms.rst:101
msgid "Whenever an Instance is started or stopped and it's ``setup`` or ``shutdown`` phase has completed a ``configure`` phase is initiated on all running instances."
msgstr ""

#: ../deployment/opsworks/terms.rst:104
msgid "As with all recipe runs, the ``configure`` phase recipes are passed data about all the currently running Instances and their Layers so that they can automatically reconfigure themselves based on the updated state of the Stack."
msgstr ""

#: ../deployment/opsworks/terms.rst:107
msgid "For example, a load balancer may need to automatically add or remove Plone ZEO clients from it's list of active backends, a ZEO client may need to change its ZEO server or its Relstorage Memcache if configuration for those services have changed."
msgstr ""

#: ../deployment/opsworks/terms.rst:110
msgid "This ``configure`` phase, during which the current cluster state is automatically shared with all the instances, is where the orchestration magic happens."
msgstr ""

#: ../deployment/playbook-use.rst:3
msgid "Basic Use Of The Playbook"
msgstr ""

#: ../deployment/playbook-use.rst:6
msgid "Local Configuration File"
msgstr ""

#: ../deployment/playbook-use.rst:8
msgid "For a quick start, copy one of the :file:`sample-*.yml` files to :file:`local-configure.yml`."
msgstr ""

#: ../deployment/playbook-use.rst:10
msgid "The :file:`local-configure.yml` file is automatically included in the main playbook if it's found."
msgstr ""

#: ../deployment/playbook-use.rst:16
msgid "Now, edit the :file:`local-configure.yml` file to set some required variables:"
msgstr ""

#: ../deployment/playbook-use.rst:18
msgid "admin_email"
msgstr ""

#: ../deployment/playbook-use.rst:20
msgid "The server admin's email. Probably yours. This email address will receive system notices and log analysis messages."
msgstr ""

#: ../deployment/playbook-use.rst:24
msgid "plone_initial_password"
msgstr ""

#: ../deployment/playbook-use.rst:26
msgid "The initial administrative password for the Zope/Plone installation. Not the same as the server shell login."
msgstr ""

#: ../deployment/playbook-use.rst:29
msgid "muninnode_query_ips"
msgstr ""

#: ../deployment/playbook-use.rst:31
msgid "Are you going to run a Munin monitor on a separate machine? (And, if not, why not?) Specify the IP address of the monitor machine. Or ..."
msgstr ""

#: ../deployment/playbook-use.rst:36
msgid "install_muninnode"
msgstr ""

#: ../deployment/playbook-use.rst:38
msgid "Remove the \"#\" on the ``install_muninnode: no`` line if you are not using a Munin monitor."
msgstr ""

#: ../deployment/playbook-use.rst:40
msgid "You're also nearly certainly going to want to specify a Plone version via the ``plone_version`` setting. You should be able to pick any version from 4.3.x or 5.x.x. Note that the value for this variable must be quoted to make sure it's interpreted as a string."
msgstr ""

#: ../deployment/playbook-use.rst:45
msgid "Use With Vagrant"
msgstr ""

#: ../deployment/playbook-use.rst:47
msgid "If you've installed Vagrant/VirtualBox, you're ready to test. Since Vagrant manages the connection, you don't need to create a inventory file entry."
msgstr ""

#: ../deployment/playbook-use.rst:50
msgid "There is a Vagrant setup file, :file:`Vagrantfile`, included with the playbook, you may open a command-line prompt, make sure your Ansible virtualenv is activated, and type:"
msgstr ""

#: ../deployment/playbook-use.rst:59
msgid "The first time you use a \"box\" it will be downloaded. These are large downloads; expect it to take some time."
msgstr ""

#: ../deployment/playbook-use.rst:64
msgid "Instructor note: Having several students simultaneously downloading a VirtualBox over wifi or a slow connection is a nightmare. Have a plan."
msgstr ""

#: ../deployment/playbook-use.rst:68
msgid "Once you've run :program:`vagrant up`, running it again will not automatically provision the VirtualBox. In this case, that means that Ansible is not run."
msgstr ""

#: ../deployment/playbook-use.rst:71
msgid "If you change your Ansible configuration, you'll need to use:"
msgstr ""

#: ../deployment/playbook-use.rst:79
msgid "When you run ``up`` or ``provision``, watch to make sure it completes successfully. Note that failures for particular plays do not mean that Ansible provisioning failed. The playbook has some tests that fail if particular system features are unavailable. Those test failures are ignored and the provisioning continues. The provisioning has failed if an error causes it to stop."
msgstr ""

#: ../deployment/playbook-use.rst:85
msgid "An example of an ignored failure::"
msgstr ""

#: ../deployment/playbook-use.rst:93
msgid "Vagrant Ports"
msgstr ""

#: ../deployment/playbook-use.rst:95
msgid "The Vagrant setup (in :file:`Vagrantfile`) maps several ports on the guest machine (the VirtualBox) to the host box. The general scheme is to forward a host port that is 1000 greater than the guest port."
msgstr ""

#: ../deployment/playbook-use.rst:98
msgid "For example, the load-balancer monitor port on the guest server is ``1080``. On the host machine, that's mapped by ssh tunnel to 2080."
msgstr ""

#: ../deployment/playbook-use.rst:101
msgid "We may see the HAProxy monitor at ``http://localhost:2080/admin``."
msgstr ""

#: ../deployment/playbook-use.rst:103
msgid "The guest's http port (80) is reached via the host machine's port 1080 -- but that isn't actually useful due to URL rewriting for virtual hosting."
msgstr ""

#: ../deployment/playbook-use.rst:106
msgid "If you take a look at ``http://localhost:1080`` from your host machine, you'll see the default Plone site, but stylesheets, JavaScript and images will all be missing."
msgstr ""

#: ../deployment/playbook-use.rst:109
msgid "Instead, look at the load-balancer port (8080 on the guest, 9080 on the host) to see your ZODB root."
msgstr ""

#: ../deployment/playbook-use.rst:112
msgid "Some Quick Vagrant"
msgstr ""

#: ../deployment/playbook-use.rst:123
msgid "To each of the these commands, you may add an ID to pick one of the boxes defined in Vagrantfile. Read Vagrantfile for the IDs."
msgstr ""

#: ../deployment/playbook-use.rst:126
msgid "For example, ``centos7`` is the ID for a CentOS box."
msgstr ""

#: ../deployment/playbook-use.rst:133
msgid "Run Against Cloud"
msgstr ""

#: ../deployment/playbook-use.rst:135
msgid "Let's provision a cloud server. Here are the facts we need to know about our cloud server:"
msgstr ""

#: ../deployment/playbook-use.rst:138
msgid "hostname"
msgstr ""

#: ../deployment/playbook-use.rst:140
msgid "A new server may or may not have a DNS host entry. If it does, use that hostname. If not, invent one and be prepared to supply an IP address."
msgstr ""

#: ../deployment/playbook-use.rst:144
msgid "login ID"
msgstr ""

#: ../deployment/playbook-use.rst:146
msgid "The user ID of a system account that is either the superuser (root) or is allowed to use :command:`sudo` to issue arbitrary commands as the superuser."
msgstr ""

#: ../deployment/playbook-use.rst:148
msgid "password"
msgstr ""

#: ../deployment/playbook-use.rst:150
msgid "If your cloud-hosting company does not set up the user account for ssh-keypair authentication, you'll need a password. Even if your account does allow passwordless login, it may still require a password to run :command:`sudo`."
msgstr ""

#: ../deployment/playbook-use.rst:153
msgid "If your cloud-hosting company sets up a root user and password, it's a good practice to login (or use Ansible) to create a new, unprivileged user with sudo rights. Cautious sysadmins will also disable root login via ssh."
msgstr ""

#: ../deployment/playbook-use.rst:156
msgid "connection details"
msgstr ""

#: ../deployment/playbook-use.rst:158
msgid "If you don't have a DNS host record for your server, you'll need to have its IP address. If ssh is switched to an alternate port, you'll need that port number."
msgstr ""

#: ../deployment/playbook-use.rst:161
msgid "With that information, create an inventory file (if none exists) and create a host entry in it."
msgstr ""

#: ../deployment/playbook-use.rst:163
msgid "We use :file:`inventory.cfg` for an inventory file."
msgstr ""

#: ../deployment/playbook-use.rst:165
msgid "A typical inventory file::"
msgstr ""

#: ../deployment/playbook-use.rst:169
msgid "You may leave off the ``ansible_host`` if the name supplied matches the DNS host record. You may leave off the ``ansible_user`` if your user ID is the same on the server."
msgstr ""

#: ../deployment/playbook-use.rst:172
msgid "An inventory file may have many entries. You may run Ansible against one, two, all of the hosts in the inventory file, or against alias groups like \"plone-servers\"."
msgstr ""

#: ../deployment/playbook-use.rst:175
msgid "See `Ansible's inventory documentation <http://docs.ansible.com/ansible/intro_inventory.html>`_ for information on grouping host entries and for more specialized host settings."
msgstr ""

#: ../deployment/playbook-use.rst:178
msgid "Now, let's make things easier for us going forward by creating an :file:`ansible.cfg` file in our playbook directory."
msgstr ""

#: ../deployment/playbook-use.rst:180
msgid "In that text file, specify the location of your inventory file:"
msgstr ""

#: ../deployment/playbook-use.rst:189
msgid "Smoke Test"
msgstr ""

#: ../deployment/playbook-use.rst:191
msgid "Now, let's see if we can use Ansible to connect to the remote machine that we've specified in our inventory."
msgstr ""

#: ../deployment/playbook-use.rst:193
msgid "Does the new machine allow an ssh key login, then you ought to be able to use the command:"
msgstr ""

#: ../deployment/playbook-use.rst:199
msgid "If you need a password for login, try:"
msgstr ""

#: ../deployment/playbook-use.rst:205
msgid "And, if that fails, ask for verbose feedback from Ansible:"
msgstr ""

#: ../deployment/playbook-use.rst:211
msgid "Now, let's test our ability to become superuser on the remote machine."
msgstr ""

#: ../deployment/playbook-use.rst:213
msgid "If you have passwordless sudo, this should work:"
msgstr ""

#: ../deployment/playbook-use.rst:220
msgid "If sudo requires a password, try:"
msgstr ""

#: ../deployment/playbook-use.rst:227
msgid "If all that works, congratulations, you're ready to use Ansible to provision the remote machine."
msgstr ""

#: ../deployment/playbook-use.rst:231
msgid "The \"become\" flag tells Ansible to carry out the action while becoming another user on the remote machine."
msgstr ""

#: ../deployment/playbook-use.rst:233
msgid "If no user is specified, we become the superuser."
msgstr ""

#: ../deployment/playbook-use.rst:235
msgid "If no method is specified, it's done via :command:`sudo`."
msgstr ""

#: ../deployment/playbook-use.rst:237
msgid "You won't often use the ``--become`` flag because the playbooks that need it specify it themselves."
msgstr ""

#: ../deployment/playbook-use.rst:240
msgid "Diagnosing SSH Connection Failures"
msgstr ""

#: ../deployment/playbook-use.rst:242
msgid "If Ansible has trouble connecting to the remote host, you're going to get a message like:"
msgstr ""

#: ../deployment/playbook-use.rst:252
msgid "If this happens to you, try adding ``-vvv`` to the :program:`ansible` or :program:`ansible-playbook` command line."
msgstr ""

#: ../deployment/playbook-use.rst:254
msgid "The extra information may -- or may not -- be useful."
msgstr ""

#: ../deployment/playbook-use.rst:256
msgid "The real test is to use a direct ssh login to get the ssh error."
msgstr ""

#: ../deployment/playbook-use.rst:258
msgid "There's a pretty good chance that the identity of the remote host will have changed, and ssh will give you a command line to clean it up."
msgstr ""

#: ../deployment/playbook-use.rst:261
msgid "Running The Playbook"
msgstr ""

#: ../deployment/playbook-use.rst:263
msgid "We're ready to run the playbook."
msgstr ""

#: ../deployment/playbook-use.rst:265
msgid "Make sure you're logged to your ansible-playbook directory and that you've activated the Python virtualenv that includes Ansible."
msgstr ""

#: ../deployment/playbook-use.rst:267
msgid "If you're targetting all the hosts in your inventory, running the playbook may be as easy as:"
msgstr ""

#: ../deployment/playbook-use.rst:273
msgid "If you need a password for ssh login, add ``-k``."
msgstr ""

#: ../deployment/playbook-use.rst:275
msgid "If you need a password for sudo, add ``-K``."
msgstr ""

#: ../deployment/playbook-use.rst:277
msgid "If you need a password for both, add \"-k -K\"."
msgstr ""

#: ../deployment/playbook-use.rst:279
msgid "If you want to target a particular host in your inventory, add ``--limit=hostname``. Note that the ``--limit`` parameter is a search term; all hostnames matching the parameter will run."
msgstr ""

#: ../deployment/playbook-use.rst:283
msgid "As with Vagrant, check the last message to make sure it completes successfully. When first provisioning a server, timeout errors are more likely."
msgstr ""

#: ../deployment/playbook-use.rst:286
msgid "If you have a timeout, run the playbook again. Note that failures for particular plays do not mean that Ansible provisioning failed."
msgstr ""

#: ../deployment/playbook-use.rst:290
msgid "Firewalling"
msgstr ""

#: ../deployment/playbook-use.rst:292
msgid "Running the Plone playbook does not set up server firewalling. That's handled via a separate playbook, included with the kit."
msgstr ""

#: ../deployment/playbook-use.rst:295
msgid "We've separated the functions because many sysadmins will wish to handle firewalling themselves."
msgstr ""

#: ../deployment/playbook-use.rst:297
msgid "If you wish to use our firewall playbook, use the command:"
msgstr ""

#: ../deployment/playbook-use.rst:303
msgid ":file:`firewall.yml` is a dispatcher. Actual firewall code is in the :file:`firewalls` subdirectory and is platform-specific. ``ufw`` is used for the Debian-family; ``firewalld``"
msgstr ""

#: ../deployment/playbook-use.rst:307
msgid "The general firewall strategy is to block everything but the ports for ssh, http, https and munin-node. The munin-node port is restricted to the monitor IP you specify."
msgstr ""

#: ../deployment/playbook-use.rst:312
msgid "This strategy assumes that you're going to use ssh tunnelling if you need to connect to other ports."
msgstr ""

#: ../deployment/plone-playbook.rst:3
msgid "The Plone Playbook"
msgstr ""

#: ../deployment/plone-playbook.rst:6
msgid "Supported Platforms"
msgstr ""

#: ../deployment/plone-playbook.rst:8
msgid "We support two Linux families: Debian and RHEL. *Support* means that the playbook knows how to load platform package dependencies and how to set up users, groups, and the platform's method for setting up daemons to start and stop with the operating system."
msgstr ""

#: ../deployment/plone-playbook.rst:13
msgid "There's no particular reason why we can't extend that support to other families, like BSD. All we need is a champion to take responsibility for extending and testing on other platforms."
msgstr ""

#: ../deployment/plone-playbook.rst:16
msgid "Debian"
msgstr ""

#: ../deployment/plone-playbook.rst:18
msgid "Our goal is to support the current Ubuntu LTS and the Debian equivalent. Currently we're doing a bit better than that. On Ubuntu we're supporting everything from Trusty to Xenial. On Debian, we're working with both Jessie and Wheezy."
msgstr ""

#: ../deployment/plone-playbook.rst:23
msgid "RHEL"
msgstr ""

#: ../deployment/plone-playbook.rst:25
msgid "We're currently only testing on CentOS 7. If you're using Plone on RHEL, we could use your help on extending that support."
msgstr ""

#: ../deployment/plone-playbook.rst:29
msgid "Quick review of contents"
msgstr ""

#: ../deployment/plone-playbook.rst:31
msgid "Let's review what you're getting when you check out the Plone Ansible Playbook."
msgstr ""

#: ../deployment/plone-playbook.rst:36
msgid "We include two playbooks:"
msgstr ""

#: ../deployment/plone-playbook.rst:38
msgid "playbook.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:40
msgid "The main playbook that sets everything except the firewall."
msgstr ""

#: ../deployment/plone-playbook.rst:42
msgid "firewall.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:44
msgid "A separate playbook to set up the software firewall. Most sysadmins have their own firewall experience, and may or may not choose to use this playbook."
msgstr ""

#: ../deployment/plone-playbook.rst:48
msgid "roles"
msgstr ""

#: ../deployment/plone-playbook.rst:50
msgid "Roles are basically pre-packaged subroutines with their own default variables. Several roles are part of the Plone Ansible Playbook kit and will be present in your initial checkout. These include roles that set up the HAProxy load balancer, varnish cache, nginx http server, postfix SMTP agent, munin-node monitoring, logwatch log analysis, message-of-the-day and a fancy setup for restarting ZEO clients."
msgstr ""

#: ../deployment/plone-playbook.rst:54
msgid "Other roles, including the role that actually sets up Plone, are loaded when you use ``ansible-galaxy`` to fetch the items listed in :file:`requirements.yml`. Except for the Plone server role, these are generally generic Ansible Galaxy roles that we liked."
msgstr ""

#: ../deployment/plone-playbook.rst:58
msgid "Vagrant"
msgstr ""

#: ../deployment/plone-playbook.rst:60
msgid "Vagrant/VirtualBox is a handy way to test your playbook, both during development and for future maintenance. We include a couple of files to help you get started with Vagrant testing."
msgstr ""

#: ../deployment/plone-playbook.rst:63
msgid "Vagrantfile"
msgstr ""

#: ../deployment/plone-playbook.rst:65
msgid "A Vagrant setup file that will allow you to create guest virtual hosts for any of the platforms we support and will run Ansible as the provisioner with playbook.yml. This defaults to building a Xenial box, but you may pick others by naming them on the :command:`vagrant up` command line."
msgstr ""

#: ../deployment/plone-playbook.rst:68
msgid "vbox_host.cfg"
msgstr ""

#: ../deployment/plone-playbook.rst:70
msgid "When you use vagrant commands, vagrant controls the ssh connection. :file:`vbox_host.cfg` is an Ansible inventory file that should allow you to run your playbook directly (without the :command:`vagrant` command) against your guest box."
msgstr ""

#: ../deployment/plone-playbook.rst:74
msgid "Sample configurations"
msgstr ""

#: ../deployment/plone-playbook.rst:76
msgid "The playbook kit contains several sample configuration files."
msgstr ""

#: ../deployment/plone-playbook.rst:78
msgid "sample-very-small.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:80
msgid "Targets a server with 512MB of memory and one CPU core. Sets up one ZEO client with two threads with small object caches. No load balancer. Varnish cache is file-based."
msgstr ""

#: ../deployment/plone-playbook.rst:85
msgid "sample-small.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:87
msgid "Targets a server with 1GB of memory and one CPU core. Sets up one ZEO client with two threads with small object caches. No load balancer. Varnish cache is file-based."
msgstr ""

#: ../deployment/plone-playbook.rst:92
msgid "sample-medium.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:94
msgid "Targets a server with 2GB of memory and two CPU cores. Sets up two ZEO clients, each with one thread with a medium object cache. Uses load balancer to manage the queue to the ZEO clients. Varnish cache is memory-based."
msgstr ""

#: ../deployment/plone-playbook.rst:99
msgid "sample-multiserver.yml"
msgstr ""

#: ../deployment/plone-playbook.rst:101
msgid "A configuration that demonstrates how to run multiple Zope/Plone installs with different versions and virtual hosting."
msgstr ""

#: ../deployment/plone-playbook.rst:103
msgid "The first four samples are meant to be immediately useful. Copy and customize. The multiserver sample is a demonstration of several customization techniques. Read it for examples, but don't expect to use it without substantial customization."
msgstr ""

#: ../deployment/plone-playbook.rst:108
msgid "Why no ``sample-large.yml``? Because a larger server installation is always going to require more thought and customization. We'll discuss those customization points later. The ``sample-medium.yml`` file will give you a starting point."
msgstr ""

#: ../deployment/plone-playbook.rst:114
msgid "Tests"
msgstr ""

#: ../deployment/plone-playbook.rst:116
msgid "You'll find a :file:`tests.py` program file and a :file:`tests` directory. The :file:`tests` directory contains Doctest files to test our sample configurations. You may add your own."
msgstr ""

#: ../deployment/plone-playbook.rst:120
msgid "The :file:`tests.py` program is a convenience script that will run one or more of the Vagrant boxes against one or more of the Doctest files. Run it with no command line argument for usage help. Or, read the source ;)"
msgstr ""

#: ../deployment/plone-stack.rst:3
msgid "Intro To Plone Stack"
msgstr ""

#: ../deployment/plone-stack.rst:5
msgid "If you haven't read the first couple of chapters of `Guide to deploying and installing Plone in production <https://docs.plone.org/manage/deploying/index.html>`_, take a moment to do so."
msgstr ""

#: ../deployment/plone-stack.rst:8
msgid "You'll want to be familiar with the main components of a typical Plone install for deployment and know when each is vital and when unnecessary."
msgstr ""

#: ../deployment/plone-stack.rst:13
msgid "The generic components of a full-stack Plone installation. Not all are always used."
msgstr ""

#: ../deployment/plone-stack.rst:15
msgid "The Plone Ansible Playbook makes choices for each generic component."
msgstr ""

#: ../deployment/plone-stack.rst:21
msgid "The specific components used in Plone's Ansible Playbook."
msgstr ""

#: ../deployment/plone-stack.rst:23
msgid "**You are not stuck with our choices!**"
msgstr ""

#: ../deployment/plone-stack.rst:25
msgid "If, for example, you wish to use Apache rather than Nginx for the web server component, that won't be a particular problem. You'll need to do more work to customize."
msgstr ""

