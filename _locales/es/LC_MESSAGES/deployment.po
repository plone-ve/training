# SOME DESCRIPTIVE TITLE.
# Copyright (C) The text and illustrations in this website are licensed by the Plone Foundation under a Creative Commons Attribution 4.0 International license.
# This file is distributed under the same license as the Plone Training package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Plone Training 1.2.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-11-02 12:20-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Jesús Delgado <talpiod@gmail.com>, 2016\n"
"Language-Team: Spanish (https://www.transifex.com/plone/teams/21152/es/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: es\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

#: ../deployment/ansible.rst:3
msgid "Intro to Ansible"
msgstr ""

#: ../deployment/ansible.rst:5
msgid ""
"Ansible is an open-source configuration management, provisioning and "
"application deployment platform written in Python and using YAML (YAML Ain't"
" Markup Language) as a configuration language. Ansible makes its connections"
" from your computer to the target machine using SSH."
msgstr ""

#: ../deployment/ansible.rst:8
msgid ""
"There is no server-side component other than an SSH server. General "
"familiarity with SSH is very desirable if you're using Ansible -- as well as"
" being a baseline skill for server administration."
msgstr ""

#: ../deployment/ansible.rst:12
msgid "Installation"
msgstr "Instalación"

#: ../deployment/ansible.rst:14
msgid ""
"Ansible is usually installed on the orchestrating computer -- typically your"
" desktop or laptop. It is a large Python application (though a fraction the "
"size of Plone!) that needs many specific Python packages from the Python "
"Package Index (PyPI)."
msgstr ""

#: ../deployment/ansible.rst:17
msgid ""
"That makes Ansible a strong candidate for a Python :program:`virtualenv` "
"installation If you don't have :program:`virtualenv` installed on your "
"computer, do it now."
msgstr ""

#: ../deployment/ansible.rst:20
msgid ""
":program:`virtualenv` may be installed via an OS package manager, or on a "
"Linux or BSD machine with the command:"
msgstr ""

#: ../deployment/ansible.rst:26
msgid ""
"Once you've got :program:`virtualenv`, use it to create a working directory "
"containing a virtual Python:"
msgstr ""

#: ../deployment/ansible.rst:32
msgid "Then, install Ansible there:"
msgstr ""

#: ../deployment/ansible.rst:39
msgid "Now, to use Ansible, activate that Python environment."
msgstr ""

#: ../deployment/ansible.rst:48
msgid ""
"Trainers: check to make sure everyone understands the basic ``source "
"activate`` mechanism."
msgstr ""

#: ../deployment/ansible.rst:50
msgid ""
"Now, let's get a copy of the *Plone Ansible Playbook*. Make sure you're "
"logged in to your ``ansible_work`` directory."
msgstr ""

#: ../deployment/ansible.rst:53
msgid ""
"Unless you're participating in the development of the playbook, or need a "
"particular fix, you'll want to check out the ``STABLE`` branch. The "
"``STABLE`` branch is a pointer to the last release of the playbook."
msgstr ""

#: ../deployment/ansible.rst:60
msgid "Or,"
msgstr ""

#: ../deployment/ansible.rst:68
msgid ""
"That gives you the Plone Ansible Playbook. You'll also need to install a few"
" Ansible roles. Roles are Ansible playbooks packaged for distribution. "
"Fortunately, you may pick up everything with a single command."
msgstr ""

#: ../deployment/ansible.rst:78
msgid ""
"If you forget that command, it's in the short README.rst file in the "
"playbook."
msgstr ""

#: ../deployment/ansible.rst:82
msgid ""
"The rationale for checking the Plone Ansible Playbook out inside the "
"virtualenv directory is that it ties the two together. Months from now, "
"you'll know that you can use the playbook with the Python and Ansible "
"packages in the virtualenv directory. We check out the playbook as a "
"subdirectory of the virtualenv directory so that we can search our playbooks"
" and roles without having to search the whole virtualenv set of packages."
msgstr ""

#: ../deployment/ansible.rst:87
msgid "Ansible basics"
msgstr ""

#: ../deployment/ansible.rst:90
msgid "Connecting to remote machines"
msgstr ""

#: ../deployment/ansible.rst:92
msgid "To use Ansible to provision a remote server, we have two requirements:"
msgstr ""

#: ../deployment/ansible.rst:94
msgid ""
"We must be able to connect to the remote machine using :command:`ssh`; and,"
msgstr ""

#: ../deployment/ansible.rst:96
msgid ""
"We must be able to issue commands on the remote server as ``root`` "
"(superuser), usually via :command:`sudo`."
msgstr ""

#: ../deployment/ansible.rst:98
msgid ""
"You'll need to familiarize yourself with how to fulfill these requirements "
"on the cloud/virtual environment of your choice. Examples:"
msgstr ""

#: ../deployment/ansible.rst:101
msgid "Using Vagrant/virtualbox"
msgstr ""

#: ../deployment/ansible.rst:103
msgid ""
"You will initially be able to log in as the \"vagrant\" user using a private"
" key that's in a file created by Vagrant. The user \"vagrant\" may issue "
":command:`sudo` commands with no additional password."
msgstr ""

#: ../deployment/ansible.rst:106
msgid "Using Linode"
msgstr ""

#: ../deployment/ansible.rst:108
msgid ""
"You'll set a root password when you create your new machine. If you're "
"willing to use the root user directly, you will not need a :command:`sudo` "
"password."
msgstr ""

#: ../deployment/ansible.rst:111
msgid "When setting up a Digital Ocean machine"
msgstr ""

#: ../deployment/ansible.rst:113
msgid ""
"New machines are typically created with a root account that contains your "
"ssh public key as an authorized key."
msgstr ""

#: ../deployment/ansible.rst:116
msgid "AWS"
msgstr ""

#: ../deployment/ansible.rst:118
msgid ""
"AWS EC2 instances are typically created with a an account named \"root\" or "
"a short name for the OS, like \"ubuntu\", that contains your ssh public key "
"as an authorized key. Passwordless :command:`sudo` is pre-enabled for that "
"account."
msgstr ""

#: ../deployment/ansible.rst:121
msgid ""
"The most important thing is that you know your setup. Test that knowledge by"
" trying an ssh login and issuing a superuser command."
msgstr ""

#: ../deployment/ansible.rst:132
msgid "Inventories"
msgstr ""

#: ../deployment/ansible.rst:134
msgid ""
"Ansible is usually run on a local computer, and it usually acts on one or "
"more remote machines. We tell Ansible how to connect to remote machines by "
"maintaining a text inventory file."
msgstr ""

#: ../deployment/ansible.rst:137
msgid ""
"There is a sample inventory configuration file in your distribution. It's "
"meant for use with a Vagrant-style virtualbox."
msgstr ""

#: ../deployment/ansible.rst:148
msgid ""
"This inventory file is complicated by the fact that a virtualbox typically "
"has no DNS host name and uses a non-standard port and a special SSH key "
"file. So, we have to specify all those things."
msgstr ""

#: ../deployment/ansible.rst:151
msgid ""
"If we were using a DNS-known hostname and our standard ssh key files, it "
"could be much simpler:"
msgstr ""

#: ../deployment/ansible.rst:157
msgid ""
"Ansible inventory files may list multiple hosts and may have aliases for "
"groups of hosts. See https://docs.ansible.com for details."
msgstr ""

#: ../deployment/ansible.rst:160 ../deployment/plone_playbook.rst:34
msgid "Playbooks"
msgstr ""

#: ../deployment/ansible.rst:162
msgid ""
"We're going to cover just enough on Ansible playbooks to allow you to read "
"and customize Plone's playbook. `Ansible's documentation "
"<http://docs.ansible.com>`_ is excellent if you want to learn more."
msgstr ""

#: ../deployment/ansible.rst:165
msgid ""
"In Ansible, an individual instruction for the setup of the remote server is "
"called a _task_. Here's a task that makes sure a directory exists."
msgstr ""

#: ../deployment/ansible.rst:176
msgid ""
"This uses the Ansible ``file`` module to check to see if a directory exists "
"with the designated mode. If it doesn't, it's created."
msgstr ""

#: ../deployment/ansible.rst:179
msgid ""
"Tasks may also have execution conditions expressed in Python syntax and may "
"iterate over simple data structures."
msgstr ""

#: ../deployment/ansible.rst:181
msgid ""
"In addition to tasks, Ansible's basic units are *host* and *variable* "
"specifications."
msgstr ""

#: ../deployment/ansible.rst:183
msgid ""
"An Ansible *playbook* is a specification of tasks that are executed for "
"specified hosts and variables. All of these specifications are in YAML."
msgstr ""

#: ../deployment/ansible.rst:187
msgid "Quick intro to YAML"
msgstr ""

#: ../deployment/ansible.rst:189
msgid ""
"YAML isn't a markup language, and it isn't a programming language either. "
"It's a data-specification notation. Just like JSON. Except that YAML -- very"
" much unlike JSON -- is meant to be written and read by humans. The creators"
" of YAML call it a \"human friendly data serialization standard\"."
msgstr ""

#: ../deployment/ansible.rst:197
msgid ""
"YAML is actually a superset of JSON. Every JSON file is also a valid YAML "
"file. But if we just fed JSON to the YAML parser, we'd be missing the point "
"of YAML, which is human readability."
msgstr ""

#: ../deployment/ansible.rst:201
msgid ""
"Basic types available in YAML include strings, booleans, floating-point "
"numbers, integers, dates, times and date-times. Structured types are "
"sequences (lists) and mappings (dictionaries)."
msgstr ""

#: ../deployment/ansible.rst:204
msgid "Sequences are indicated by list-member lines with leading dashes:"
msgstr ""

#: ../deployment/ansible.rst:212
msgid ""
"Mappings are indicated with key/value pairs with colons separating keys and "
"values:"
msgstr ""

#: ../deployment/ansible.rst:220
msgid "Complex data structures are designated with indentation:"
msgstr ""

#: ../deployment/ansible.rst:244
msgid "Basic types read as you'd expect:"
msgstr ""

#: ../deployment/ansible.rst:255
msgid "Finally, remember that this is a superset of JSON:"
msgstr ""

#: ../deployment/ansible.rst:262
msgid ""
"Want to turn YAML into Python data structures? Or Python into YAML? Python "
"has several YAML parser/generators. The most commonly used is PyYAML."
msgstr ""

#: ../deployment/ansible.rst:267
msgid ""
"Quick code to read YAML from the standard input and turn it into pretty-"
"printed Python data:"
msgstr ""

#: ../deployment/ansible.rst:273
msgid "Quick intro to Jinja2"
msgstr ""

#: ../deployment/ansible.rst:275
msgid ""
"YAML doesn't have any built-in way to read a variable. Ansible uses the "
"Jinja2 templating language for this purpose."
msgstr ""

#: ../deployment/ansible.rst:278
msgid ""
"A quick example: Let's say we have a variable :py:data:`timezone` containing"
" the target server's desired timezone setting. We can use that variable in a"
" task via Jinja2's double-brace notation: ``{{ timezone }}``."
msgstr ""

#: ../deployment/ansible.rst:281
msgid ""
"Jinja2 also supports limited Python expression syntax and can read object "
"properties or mapping key/vaues with a dot notation::"
msgstr ""

#: ../deployment/ansible.rst:286
msgid ""
"There are also various filters and tests available via a pipe notation. For "
"example, we use the ``default`` filter to supply a default value if a "
"variable is undefined."
msgstr ""

#: ../deployment/ansible.rst:300
msgid ""
"Jinja2 also is used as a full templating language whenever we need to treat "
"a text file as a template to fill in variable values or execute loops or "
"branching logic. Here's an example from the template used to construct a "
"buildout.cfg:"
msgstr ""

#: ../deployment/ansible.rst:314
msgid "Playbook structure"
msgstr ""

#: ../deployment/ansible.rst:316
msgid ""
"An Ansible \"play\" is a mapping (or dictionary) with keys for hosts, "
"variables and tasks. A playbook is a sequence of such dictionaries."
msgstr ""

#: ../deployment/ansible.rst:319
msgid "A simple playbook:"
msgstr ""

#: ../deployment/ansible.rst:329
msgid ""
"The value of hosts could be a single host name, the name of a group of "
"hosts, or \"all\"."
msgstr ""

#: ../deployment/ansible.rst:332
msgid "Variables"
msgstr ""

#: ../deployment/ansible.rst:335
msgid "Notifications and handlers"
msgstr ""

#: ../deployment/ansible.rst:337
msgid "We may also specify \"handlers\" that are run if needed."
msgstr ""

#: ../deployment/ansible.rst:354
msgid ""
"Handlers are run if a matching notification is registered. A particular "
"handler is only run once, even if several notifications for it are "
"registered."
msgstr ""

#: ../deployment/ansible.rst:358
msgid "Roles"
msgstr "Roles"

#: ../deployment/ansible.rst:360
msgid ""
"Ansible has various ways to include the contents of YAML files into your "
"playbook. \"Roles\" do it in a more structured way -- much more like a "
"package. Roles contain their own variables, tasks and handlers. They inherit"
" the global variable environment and you may pass particular variables when "
"they are called."
msgstr ""

#: ../deployment/ansible.rst:365
msgid ""
"Plone's Ansible Playbook includes several roles for chores such as setting "
"up the load balancer and web server. Other roles are fetched (the role "
"source itself is fetched) by ``ansible-galaxy`` when we use it to set up "
"requirements. Most are fetched from github."
msgstr ""

#: ../deployment/ansible.rst:369
msgid "An simple Ansible playbook using roles:"
msgstr ""

#: ../deployment/ansible.rst:390
msgid ""
"If we want to pass variables to roles, we just add their keys and values to "
"the mapping."
msgstr ""

#: ../deployment/ansible.rst:392
msgid ""
"Take a look at the ``when: install_loadbalancer|default(True)`` line above. "
"A ``when`` key in a role or task mapping sets a condition for execution. For"
" conditionals like ``when``, Ansible expects a Jinja2 expression."
msgstr ""

#: ../deployment/ansible.rst:396
msgid ""
"We could also have expressed that ``when`` condition as ``\"{{ "
"install_loadbalancer|default(True) }}\"``. Ansible interprets all literal "
"strings as little Jinja2 templates."
msgstr ""

#: ../deployment/customization.rst:3
msgid "More customized use"
msgstr ""

#: ../deployment/customization.rst:5
msgid ""
"We intend that you should be able to make most changes by changing default "
"variable settings in your ``local_configure.yml`` file. We've made a serious"
" effort to make sure that all those settings are documented in the `Plone's "
"Ansible Playbook <http://docs.plone.org/external/ansible-"
"playbook/docs/index.html>` documentation."
msgstr ""

#: ../deployment/customization.rst:8
msgid ""
"For example, if you want to change the time at which backup occurs, you can "
"check the doc and discover that we have a `plone-backup-at setting "
"<http://docs.plone.org/external/ansible-playbook/docs/plone.html#plone-"
"backup-at>`_. The default setting is:"
msgstr ""

#: ../deployment/customization.rst:18
msgid "That's 02:30 every morning."
msgstr ""

#: ../deployment/customization.rst:20
msgid "To make it 03:57 instead, use:"
msgstr ""

#: ../deployment/customization.rst:29
msgid "in your ``local_configure.yml`` file."
msgstr ""

#: ../deployment/customization.rst:32
msgid "Common customization points"
msgstr ""

#: ../deployment/customization.rst:34
msgid "Let's review the settings that are very commonly changed."
msgstr ""

#: ../deployment/customization.rst:37
msgid "Plone setup"
msgstr ""

#: ../deployment/customization.rst:40
msgid "Eggs and versions"
msgstr ""

#: ../deployment/customization.rst:42
msgid ""
"You're likely to want to add Python packages to your Plone installation to "
"enable add-on functionality."
msgstr ""

#: ../deployment/customization.rst:44
msgid ""
"Let's say you want to add :py:mod:`Products.PloneFormGen` and "
":py:mod:`webcouturier.dropdownmenu`. Just add to your "
"``local_configure.yml``:"
msgstr ""

#: ../deployment/customization.rst:53
msgid "If you add eggs, you should nearly always specify their versions:"
msgstr ""

#: ../deployment/customization.rst:62
msgid ""
"That takes care of packages that are available on the `Python Package Index "
"<https://pypi.python.org/pypi>`_. What if your developing packages via git?"
msgstr ""

#: ../deployment/customization.rst:70
msgid ""
"There's more that you can do with the ``plone_sources`` setting. See the "
"docs!"
msgstr ""

#: ../deployment/customization.rst:74
msgid "buildout from git repo"
msgstr ""

#: ../deployment/customization.rst:76
msgid ""
"It's entirely possible that the buildout created by the playbook won't be "
"adequate to your needs. If that's the case, you may check out your whole "
"buildout directory via git:"
msgstr ""

#: ../deployment/customization.rst:84
msgid ""
"Make sure you check the `documentation on this setting "
"<http://docs.plone.org/external/ansible-playbook/docs/plone.html#plone-"
"buildout-git-repo>`_. Even if you use your own buildout, you'll need to make"
" sure that some of the playbook settings reflect your configuration."
msgstr ""

#: ../deployment/customization.rst:88
msgid "Running buildout and restarting clients"
msgstr ""

#: ../deployment/customization.rst:90
msgid ""
"By default, the playbook tries to figure out if :command:`buildout` needs to"
" be run. If you add an egg, for example, the playbook will run buildout to "
"make the buildout-controlled portions of the installation update."
msgstr ""

#: ../deployment/customization.rst:93
msgid "If you don't want that behavior, change it:"
msgstr ""

#: ../deployment/customization.rst:99
msgid ""
"If ``autorun`` is turned off, you'll need to log in to run buildout after it"
" completes the first time. (When you first run the playbook on a new server,"
" buildout will always run.)"
msgstr ""

#: ../deployment/customization.rst:102
msgid ""
"If automatically running buildout bothers you, automatically restarting "
"Plone after running buildout will seem foolish. You may turn it off:"
msgstr ""

#: ../deployment/customization.rst:109
msgid ""
"That gives you the option to log in and run the client restart script. If "
"you're conservative, you'll first try starting and stopping the reserved "
"client."
msgstr ""

#: ../deployment/customization.rst:115
msgid ""
"By the way, if buildout fails, your playbook run will halt. So, you don't "
"need to worry that an automated restart might occur after a failed buildout."
msgstr ""

#: ../deployment/customization.rst:120
msgid "Web hosting options"
msgstr ""

#: ../deployment/customization.rst:122
msgid ""
"It's very likely that you're going to need to make some changes in nginx "
"configuration. Most of those changes are made via the "
"``webserver_virtualhosts`` setting."
msgstr ""

#: ../deployment/customization.rst:125
msgid ""
"``webserver_virtualhosts`` should contain a list of the hostnames you wish "
"to support. For each one of those hostnames, you may make a variety of setup"
" changes."
msgstr ""

#: ../deployment/customization.rst:128
msgid ""
"The playbook automatically creates a separate host file for each host you "
"configure."
msgstr ""

#: ../deployment/customization.rst:130
msgid "Here's the default setting:"
msgstr ""

#: ../deployment/customization.rst:139
msgid ""
"This connects your inventory hostname for the server to the /Plone directory"
" in the ZODB."
msgstr ""

#: ../deployment/customization.rst:141
msgid "A more realistic setting might look something like:"
msgstr ""

#: ../deployment/customization.rst:162
msgid ""
"Here we're setting up two separate hosts, one for http and one for https. "
"Both point to the same ZODB path, though they don't have to. The https host "
"item also refers to a key/certificate file pair on the Ansible host machine."
" They'll be copied to the remote server."
msgstr ""

#: ../deployment/customization.rst:167
msgid ""
"Alternatively, you could specify use of certificates already on the server:"
msgstr ""

#: ../deployment/customization.rst:180
msgid ""
"One hazard for the current playbook web server support is that it does "
"**not** delete old host files. So, if you had previously set up "
"``www.mynewclient.com`` and then deleted that item from the playbook host "
"list, the nginx host file would remain. Log in and delete it if needed. Yes,"
" this is an exception to the \"don't login to change configuration rule\"."
msgstr ""

#: ../deployment/customization.rst:185
msgid "**Extra tricks**"
msgstr ""

#: ../deployment/customization.rst:187
msgid ""
"There are a couple of extra setting that allow you to do extra customization"
" if you know nginx directives. For example:"
msgstr ""

#: ../deployment/customization.rst:196
msgid ""
"This is a *redirect to https*. It takes advantage of the fact that if you do"
" not specify a zodb_path, the playbook will not automatically create a "
"location stanza with a rewrite and proxy_pass directives."
msgstr ""

#: ../deployment/customization.rst:200
msgid "Mail relay"
msgstr ""

#: ../deployment/customization.rst:202
msgid ""
"Some cloud server companies do not allow servers to directly send mail to "
"standard mail ports. Instead, they require that you use a *mail relay*. This"
" is a typical setup:"
msgstr ""

#: ../deployment/customization.rst:214
msgid "Bypassing components"
msgstr ""

#: ../deployment/customization.rst:216
msgid ""
"Remember our stack diagram? The only part of the stack that you're stuck "
"with is Plone. All the other components my be replaced. To replace them, "
"first prevent the playbook from installing the default component. Then, use "
"a playbook of your own to install the alternative component."
msgstr ""

#: ../deployment/customization.rst:222
msgid ""
"For example, to install an alternative to the Postfix mail agent, just add:"
msgstr ""

#: ../deployment/customization.rst:230
msgid ""
"If you choose not to install the haproxy, varnish or nginx, you take on some"
" extra responsibilities. You're going to need to make sure in particular "
"that your port addresses match up. If, for example, you replace haproxy, you"
" will need to point varnish to the new load-balancer's frontend. You'll need"
" to point the new load balancer to the ZEO clients."
msgstr ""

#: ../deployment/customization.rst:236
msgid "Multiple Plones per host"
msgstr ""

#: ../deployment/customization.rst:238
msgid ""
"So far, we've covered the simple case of having one Plone server installed "
"on your server. In fact, you may install additional Plones."
msgstr ""

#: ../deployment/customization.rst:241
msgid ""
"To do so, you create a list variable ``playbook_plones`` containing all the "
"settings that are specific to one or more of your Plone instances."
msgstr ""

#: ../deployment/customization.rst:243
msgid ""
"Nearly all the plone_* variables, and a few others like loadbalancer_port "
"and webserver_virtualhosts may be set in playbook_plones. Here's a simple "
"example:"
msgstr ""

#: ../deployment/customization.rst:266
msgid ""
"Note that you're going to have to specify a minimum of an instance name, a "
"zeo port and a client base port (the address of client1 for this Plone "
"instance.)"
msgstr ""

#: ../deployment/customization.rst:268
msgid ""
"You may specify up to four items in your ``playbook_plones`` list. If you "
"need more, see the docs as you'll need to make a minor change in the main "
"playbook."
msgstr ""

#: ../deployment/customization.rst:272
msgid "The Plone Role -- using it independently"
msgstr ""

#: ../deployment/customization.rst:274
msgid ""
"Finally, for really big changes, you may find that the full playbook is of "
"little or no use. In that case, you may still wish to use Plone's Ansible "
"Role independently, in your own playbooks. The `Plone server role "
"<https://github.com/plone/ansible.plone_server>`_ is maintained "
"separately, and may become a role in your playbooks if it works for you."
msgstr ""

#: ../deployment/full.rst:3
msgid "Helping develop Plone's Ansible tools"
msgstr ""

#: ../deployment/in_operation.rst:3
msgid "In operation"
msgstr ""

#: ../deployment/in_operation.rst:5
msgid ""
"Hopefully, you've got a provisioned server. Do a quick check by ssh'ing to "
"the server. You should see a welcome message like:"
msgstr ""

#: ../deployment/in_operation.rst:28
msgid ""
"This gives you a list of all the long-lived services installed by the "
"playbook and the interface/ports at which they're attached."
msgstr ""

#: ../deployment/in_operation.rst:30
msgid ""
"Note the service addresses which begin with ``127.0.0.1``. Those services "
"should only answer requests from the server itself: from the localhost. See "
"the firewalling section below for help on tightening this up."
msgstr ""

#: ../deployment/in_operation.rst:34
msgid "So, how do you connect to local-only ports. Use ssh tunnels."
msgstr ""

#: ../deployment/in_operation.rst:41
msgid ""
"This is a pretty typical login that creates handy tunnels between ports on "
"your local machine with matching haproxy-admin, varnish and haproxy front-"
"end ports on the remote server."
msgstr ""

#: ../deployment/in_operation.rst:43
msgid ""
"While you're logged in, check out the status of the :program:`supervisor` "
"process-control system, which is used to launch your Zope/Plone processes."
msgstr ""

#: ../deployment/in_operation.rst:49
msgid "will list all the processes controlled by supervisor."
msgstr ""

#: ../deployment/in_operation.rst:52
msgid "Plone setup and directories"
msgstr ""

#: ../deployment/in_operation.rst:54
msgid "While you're logged in, let's take a look at the Plone/Zope setup."
msgstr ""

#: ../deployment/in_operation.rst:56
msgid ""
"You may modify the Zope/Plone directory layout created by the playbook. "
"Unless you do, the Playbook will put Plone's programs and configuration "
"files in ``/usr/local`` by Plone version. Data files will be in "
"``/var/local``. This split is intended to make it easier to organize backups"
" and to put data on a different physical or logical device."
msgstr ""

#: ../deployment/in_operation.rst:61
msgid ""
"Unless you change it, backups are also under ``/var/local``. It's easy to "
"change this, and it's not a bad idea to have backups on a different device."
msgstr ""

#: ../deployment/in_operation.rst:64
msgid ""
"In terms of file ownership and permissions, the Playbook pretty much follows"
" the practices of the Plone Unified Installer. Program and configuration "
"files are owned by the ``plone_buildout`` user, and data, log and backup "
"files are owned by the ``plone_daemon`` user. A ``plone_group`` is used to "
"give some needed communication, particularly the ability of buildout to "
"create directories in the data space."
msgstr ""

#: ../deployment/in_operation.rst:68
msgid ""
"This means that if you need to run ``bin/buildout`` via login, it must be "
"run as the ``plone_buildout`` user."
msgstr ""

#: ../deployment/in_operation.rst:74
msgid ""
"Typically, you would never start the main ZEO server or its clients "
"directly. That's handled via :program:`supervisorctl`. There's one exception"
" to this rule: the playbook creates a ZEO client named ``client_reserved`` "
"that is not part of the load-balancer pool and is not managed by supervisor."
" The purpose of this extra client is to allow you to handle run scripts or "
"debug starts without affecting the load-balanced client pool. It's "
"particularly a good idea to use this mechanism to test an updated buildout:"
msgstr ""

#: ../deployment/in_operation.rst:85
msgid "Restart script"
msgstr ""

#: ../deployment/in_operation.rst:87
msgid ""
"Still logged in? Let's take a look at another part of the install: the "
"restart script. Look in your buildout directory for the scripts directory. "
"In it, you should find ``restart_clients.sh``. (Go ahead and log out if "
"you're still connected.)"
msgstr ""

#: ../deployment/in_operation.rst:93
msgid ""
"This script, which needs to be run as the superuser via :program:`sudo`, is "
"intended to manage hot restarts. Its general strategy is to run through your"
" ZEO clients, sequentially doing the following:"
msgstr ""

#: ../deployment/in_operation.rst:96
msgid "Mark it down for maintenance in haproxy;"
msgstr ""

#: ../deployment/in_operation.rst:97
msgid "stop client;"
msgstr ""

#: ../deployment/in_operation.rst:98
msgid "start client; wait long enough for it to start listening"
msgstr ""

#: ../deployment/in_operation.rst:99
msgid ""
"Fetch the homepage directly from the client to load the cache. This will be "
"the first request the client receives, since haproxy hasn't have marked it "
"live yet. So, when haproxy marks it live, the cache will be warm."
msgstr ""

#: ../deployment/in_operation.rst:103
msgid "Mark the client available in haproxy."
msgstr ""

#: ../deployment/in_operation.rst:105
msgid "After running through the clients, it flushes the varnish cache."
msgstr ""

#: ../deployment/in_operation.rst:107
msgid ""
"This is only really useful if you're running multiple ZEO and using haproxy "
"for your load balancer."
msgstr ""

#: ../deployment/in_operation.rst:110
msgid "Client logs"
msgstr ""

#: ../deployment/in_operation.rst:112
msgid ""
"Unless you change it, the playbook sets up the clients to maintain 5 "
"generations of event and access logs. Event logs are rotated at 5MB, access "
"logs at 20MB."
msgstr ""

#: ../deployment/in_operation.rst:116
msgid "cron jobs"
msgstr ""

#: ../deployment/in_operation.rst:118
msgid ""
"The playbook automatically creates :command:`cron` jobs for ZODB backup and "
"packing. These jobs are run as ``plone_daemon``."
msgstr ""

#: ../deployment/in_operation.rst:121
msgid ""
"The jobs are run in the early morning in the server's time zone. Backup is "
"run daily; packing weekly."
msgstr ""

#: ../deployment/in_operation.rst:125
msgid "Load balancing"
msgstr ""

#: ../deployment/in_operation.rst:127
msgid "Let's step up the delivery stack."
msgstr ""

#: ../deployment/in_operation.rst:129
msgid ""
"All but the smallest sample playbooks set up ZEO load balancing via haproxy."
" One of the things we gain from haproxy is good reporting."
msgstr ""

#: ../deployment/in_operation.rst:132
msgid ""
"The web interface for the haproxy monitor is deliberately not available to a"
" remote connection. It's easy to get around that with an ssh tunnel:"
msgstr ""

#: ../deployment/in_operation.rst:139
msgid ""
"Now we may ask for the web report at ``http://localhost:1080/admin``. Since "
"we're restricting access, we don't bother with a password."
msgstr ""

#: ../deployment/in_operation.rst:145
msgid "Haproxy monitor at http://localhost:1080/admin"
msgstr ""

#: ../deployment/in_operation.rst:147
msgid ""
"If your optimizing, it's a great idea to look at the haproxy stats to see "
"what kind of queues are building up in your ZEO client cluster."
msgstr ""

#: ../deployment/in_operation.rst:149
msgid ""
"A word about the cluster strategy. We set up our clients with a single ZODB "
"connection thread. There's a trade-off here. Python's threading isn't great "
"on multi-core machines. If you've got only one CPU core available, that's "
"fine. But modern servers typically have several cores; this scheme allows us"
" to keep those cores more busy than they would be otherwise. The cost is "
"somewhat more memory use: a ZEO client with multiple threads does some "
"memory sharing between threads. It's not a lot, but that gives it some "
"memory use advantage over multiple, single-threaded clients. You may want to"
" make that trade off differently."
msgstr ""

#: ../deployment/in_operation.rst:159
msgid ""
"We also have haproxy set up to only make one connection at a time to each of"
" our ZEO clients. This is also a trade off. We lose the nice client behavior"
" of automatically using different delivery threads for blobs. But, we lower "
"the risk that a request will sit for a long time in an individual client's "
"queue (the client's connection queue, note haproxy's). If someone makes a "
"request that will take several seconds to render and return, we'd like to "
"avoid slowing down the response to other requests."
msgstr ""

#: ../deployment/in_operation.rst:166
msgid "Reverse-proxy caching"
msgstr ""

#: ../deployment/in_operation.rst:168
msgid ""
"We use Varnish for reverse-proxy caching. The size of the cache and its "
"storage strategy is customizable."
msgstr ""

#: ../deployment/in_operation.rst:171
msgid ""
"By default, we set up 512MB caches. That's probably about right if you're "
"using a CDN, but may be low if if your site is large and you're not using a "
"CDN. The two small samples use Varnish's ``file`` method for cache storage. "
"The larger samples use ``malloc``."
msgstr ""

#: ../deployment/in_operation.rst:176
msgid ""
"Varnish's control channel is limited to use by localhost and has no secret."
msgstr ""

#: ../deployment/in_operation.rst:178
msgid ""
"In a multi-Plone configuration, where you set up multiple, separate Plone "
"servers with separate load-balancing front ends, our VCL setup does the "
"dispatching to the different front ends."
msgstr ""

#: ../deployment/in_operation.rst:181
msgid "Web hosting"
msgstr ""

#: ../deployment/in_operation.rst:183
msgid ""
"We use nginx for the outer web server, depending on it to do efficient URL "
"rewriting for virtual hosting and for handling https."
msgstr ""

#: ../deployment/in_operation.rst:185
msgid ""
"We'll have much more to say about virtual hosting later when we talk about "
"how to customize it. What you need to know now is that simple virtual "
"hosting is automatically set up between the hostname you supply in the "
"inventory and the ``/Plone`` site in the ZODB. So, you should be able to "
"immediately ask for your server via http and get a Plone welcome page."
msgstr ""

#: ../deployment/in_operation.rst:189
msgid ""
"If your inventory hostname does not have a matching DNS host record, you're "
"going to see something like:"
msgstr ""

#: ../deployment/in_operation.rst:194
msgid "Typical virtual hosting error."
msgstr ""

#: ../deployment/in_operation.rst:196
msgid ""
"You're seeing a virtual-hosting setup error. The requested *page* is being "
"returned, but all the resource URLs in the page -- images, stylesheets and "
"javascript resources -- are pointing to the hostname supplied in the "
"inventory. You may fix that by supplying a DNS-valid hostname, or by setting"
" up specific virtual hosting. That's detailed below."
msgstr ""

#: ../deployment/in_operation.rst:201
msgid ""
"That's it for the delivery stack. Let's explore the other components "
"installed by the playbook."
msgstr ""

#: ../deployment/in_operation.rst:205
msgid "Postfix"
msgstr ""

#: ../deployment/in_operation.rst:207
msgid ""
"We use Postfix for our mailhost, and we set it up in a send-only "
"configuration. In this configuration, it should not accept connections from "
"the outside world."
msgstr ""

#: ../deployment/in_operation.rst:212
msgid ""
"You will probably have another SMTP agent that's the real mail exchange (MX)"
" for your domain. Make sure that server is configured to accept mail from "
"the ``FROM`` addresses in use on your Plone server. Otherwise, mail "
"exchanges that \"grey list\" may not accept mail from your Plone server."
msgstr ""

#: ../deployment/in_operation.rst:217
msgid "Updating system packages"
msgstr ""

#: ../deployment/in_operation.rst:219
msgid ""
"On Debian family Linux, the playbook sets up the server for automatic "
"installation of routine updates. We do not, however, set up an automatic "
"reboot for updates that require a system restart. So, be aware that you'll "
"need to watch for \"reboot required\" messages and schedule a reboot."
msgstr ""

#: ../deployment/in_operation.rst:224
msgid "fail2ban"
msgstr ""

#: ../deployment/in_operation.rst:226
msgid ""
"On Debian family Linux, the playbook installs ``fail2ban`` and configures it"
" to temporarily block IP addresses that repeatedly fail login attempts via "
"ssh."
msgstr ""

#: ../deployment/in_operation.rst:229
#: ../deployment/opsworks/maintenance.rst:54
msgid "Monitoring"
msgstr ""

#: ../deployment/in_operation.rst:231
msgid ""
":program:`logwatch` is installed and configured to email daily log summaries"
" to the administrative email address."
msgstr ""

#: ../deployment/in_operation.rst:234
msgid ""
"Unless you prevent it, :program:`munin-node` is installed and configured to "
"accept connections from the IP address you designate. To make use of it, "
"you'll need to install :program:`munin` on a monitoring machine."
msgstr ""

#: ../deployment/in_operation.rst:237
msgid ""
"The :program:`munin-node` install by the playbook disables many monitors "
"that are unlikely to be useful to a mostly dedicated Plone servers. It also "
"installs a Plone-specific monitor that reports resident memory usage by "
"Plone components."
msgstr ""

#: ../deployment/in_operation.rst:241
msgid "Changes philosophy"
msgstr ""

#: ../deployment/in_operation.rst:243
msgid ""
"The general philosophy for playbook use is that you make all server "
"configuration changes via Ansible. If you find yourself logging in to change"
" settings, think again. That's the road to having a server that is no longer"
" reproducible."
msgstr ""

#: ../deployment/in_operation.rst:247
msgid ""
"If you've got a significant change to make, try it first on a test server or"
" a Vagrant box."
msgstr ""

#: ../deployment/in_operation.rst:249
msgid ""
"This does not mean that you'll never want to log into the server. It just "
"means that you shouldn't do it to change configuration."
msgstr ""

#: ../deployment/index.rst:4
msgid "Automating Plone Deployment"
msgstr ""

#: ../deployment/index.rst:8 ../deployment/opsworks/index.rst:8
msgid ""
"This training is meant to be used in a course or read and worked through by "
"an individual user. Instructors should note that this makes it more "
"discursive than it would be if it was only meant for classroom use. Many "
"sections may be zipped through in a class, noting to students that the full "
"text is available for review."
msgstr ""

#: ../deployment/index.rst:13 ../deployment/opsworks/index.rst:13
msgid "Contents:"
msgstr "Contenidos:"

#: ../deployment/index.rst:29
msgid "http://docs.plone.org/manage/deploying/"
msgstr ""

#: ../deployment/index.rst:32
msgid "OpsWorks"
msgstr ""

#: ../deployment/index.rst:34
msgid ":doc:`opsworks/index`"
msgstr ""

#: ../deployment/index.rst:35
msgid ""
"Using Amazon Opsworks to orchestrate clusters for scalable/high availablity "
"deployments."
msgstr ""

#: ../deployment/intro.rst:3 ../deployment/opsworks/intro.rst:2
msgid "Introduction"
msgstr "Introducción"

#: ../deployment/intro.rst:5
msgid ""
"The subject of this training is the deployment of Plone for production "
"purposes. We will, in particular, be focusing on automating deployment using"
" tools which can target a fresh Linux server and create on it an efficient, "
"robust server."
msgstr ""

#: ../deployment/intro.rst:8
msgid ""
"That target server may be a cloud server newly created on AWS, Linode or "
"DigitalOcean. Or, it may be a virtual machine created for testing on your "
"own desk or laptop."
msgstr ""

#: ../deployment/intro.rst:11
msgid ""
"Our goal is that these deployments be *repeatable*. If we run the automated "
"deployment multiple times against multiple cloud servers, we should get the "
"same results. If we run the automated deployment against a virtual machine "
"on our laptop, we should be able to test it as if it was a matching cloud "
"server."
msgstr ""

#: ../deployment/intro.rst:15
msgid ""
"The tools we use for this purpose reflect the opinions of the Plone "
"Installer Team. *We are opinionated*. With a great many years of experience "
"administering servers and Plone, we feel we have a right to our opinions. "
"But, most importantly, we know we have to make choices and support those "
"choices."
msgstr ""

#: ../deployment/intro.rst:20
msgid "The tools we use may not be the ones you would choose."
msgstr ""

#: ../deployment/intro.rst:22
msgid ""
"They may not be the ones we would choose this month if we were starting "
"over."
msgstr ""

#: ../deployment/intro.rst:24
msgid ""
"But, they are tools widely used in the Plone community. They are well-"
"understood, and you should get few \"I've never heard of that\" complaints "
"if you ask questions of the Plone community."
msgstr ""

#: ../deployment/intro.rst:28
msgid "Our big choices"
msgstr ""

#: ../deployment/intro.rst:30
msgid "Linux"
msgstr ""

#: ../deployment/intro.rst:32
msgid ""
"BSD is great. OS X is familiar. Windows works just fine. But our experience "
"and the majority experience in the Plone community is with Linux for "
"production servers. That doesn't mean you have to use Linux for your laptop "
"or desktop; anything that runs Python is likely fine."
msgstr ""

#: ../deployment/intro.rst:38
msgid "Major distributions"
msgstr ""

#: ../deployment/intro.rst:40
msgid ""
"We're supporting two target distribution families: Debian and EL "
"(RedHat/CentOS). We're going to try to keep this working on the most recent "
"LTS (Long-Term Support release) or its equivalent."
msgstr ""

#: ../deployment/intro.rst:43
msgid "Platform packages"
msgstr ""

#: ../deployment/intro.rst:45
msgid ""
"We use platform packages whenever possible. We want the non-Plone components"
" on your server to be automatically updatable using your platform tools. If "
"a platform package is usable, we'll use it even if it isn't the newest, "
"coolest version."
msgstr ""

#: ../deployment/intro.rst:49
msgid "Ansible"
msgstr ""

#: ../deployment/intro.rst:51
msgid ""
"There are all sorts of great tools for automating deployment. People we "
"respect have chosen Puppet, Salt/Minion and lots of other tools. We chose "
"Ansible because it requires no preinstalled server component, it's written "
"in Python, and its configuration language is YAML, which is awfully easy to "
"read."
msgstr ""

#: ../deployment/intro.rst:55
msgid "And ..."
msgstr ""

#: ../deployment/intro.rst:57
msgid ""
"We'll discuss particular parts of the deployment stack in the next section."
msgstr ""

#: ../deployment/maintenance.rst:3
msgid "Maintenance strategies"
msgstr ""

#: ../deployment/maintenance.rst:5
msgid ""
"This section covers strategies for long-run maintenance of your playbook. If"
" you're successful with Plone's Ansible Playbook, you will wish to keep an "
"eye on its continued development. You may wish to be able to integrate bug "
"fixes and new features that have become part of the distribution. But, since"
" this project targets production servers, you'll wish to be very careful in "
"integrating those changes so that you minimize risk of breaking a live "
"server configuration."
msgstr ""

#: ../deployment/maintenance.rst:12
msgid "Rule 1: If it changes, test it."
msgstr ""

#: ../deployment/maintenance.rst:14
msgid ""
"Using Ansible (or other configuration-management systems) makes it easier to"
" test a whole server configuration. Make use of that fact! You may test by "
"running your playbook against a Vagrant box or against a staging server."
msgstr ""

#: ../deployment/maintenance.rst:18
msgid ""
"Make sure your test server matches the current live configuration. Copy "
"backup Plone data from the live server; restore it on the test server. Then,"
" make your changes in the playbook (or its Ansible support) and run it "
"against the test server. Only on testing success should you run against the "
"live server."
msgstr ""

#: ../deployment/maintenance.rst:24
msgid "Virtualenv"
msgstr ""

#: ../deployment/maintenance.rst:26
msgid ""
"If you followed our installation instructions, you have a Python virtualenv "
"attached to your playbook checkout. That virtualenv has its own installation"
" of Ansible. That's good, because it protects your playbook against "
"unexpected changes in the global environment -- such as Ansible being "
"updated by the OS update mechanisms."
msgstr ""

#: ../deployment/maintenance.rst:30
msgid ""
"You may need or wish to update the installation of Ansible in your "
"Virtualenv. If so, make sure you use the copy of :program:`pip` in your "
"virtualenv. Then, test running your playbook with your new Ansible."
msgstr ""

#: ../deployment/maintenance.rst:35
msgid "What belongs to the playbook and what doesn't"
msgstr ""

#: ../deployment/maintenance.rst:37
msgid ""
"The general strategy for playbook changes is to not modify anything that's "
"included with the playbook. We've gone to some trouble to make sure that you"
" can make most forseeable setup changes without touching distribution files."
msgstr ""

#: ../deployment/maintenance.rst:40
msgid ""
"The :file:`local-configure.yml` is an example of this strategy. It is "
"**not** included with the distribution files. It never will be. We will also"
" never include an :file:`inventory.cfg` file."
msgstr ""

#: ../deployment/maintenance.rst:45
msgid ""
"That means that you may safely merge changes from the STABLE branch of "
"https://github.com/plone/ansible-playbook without fear of overwriting those "
"files. You may also create new playbooks; just give them different names. "
"The extra playbooks might handle installs of extra components, firewalling, "
"user setup, whatever."
msgstr ""

#: ../deployment/maintenance.rst:50
msgid "Git forks"
msgstr ""

#: ../deployment/maintenance.rst:52
msgid ""
"But, what if you want to use version control with your own added files?"
msgstr ""

#: ../deployment/maintenance.rst:54
msgid ""
"In this case, you will wish to *fork* https://github.com/plone/ansible-"
"playbook. Add your extra files to those included with your local checkout of"
" the git fork and push upstream to your git repository. Then, occasionally "
"merge changes from the Plone github account's repository into your fork, "
"typically by rebasing from Plone's upstream repository STABLE branch. Make "
"sure you keep your added files when you do so."
msgstr ""

#: ../deployment/maintenance.rst:60
msgid "Maintenance strategies -- multiple hosts"
msgstr ""

#: ../deployment/maintenance.rst:62
msgid ""
"The :file:`local-configure.yml` file strategy makes it easy to get going "
"with Plone's playbook fast. But it breaks down if you wish to maintain "
"multiple, different hosts with the playbook. Fortunately, there's an easy "
"way to handle the problem."
msgstr ""

#: ../deployment/maintenance.rst:66
msgid ""
"Create a :file:`host_vars` directory inside your playbook directory (the one"
" containing playbook.yml). Now, inside that directory, create one file per "
"target host, each with a name that matches the inventory entry for the host,"
" plus ``.yml``. Each of these files should be the same as the local-"
"configure.yml file that would be used if this was a single host. Delete the "
"no-longer-needed :file:`local-configure.yml` file."
msgstr ""

#: ../deployment/opsworks/index.rst:4
msgid "Orchestrating Plone Deployments with Amazon OpsWorks"
msgstr ""

#: ../deployment/opsworks/installation.rst:2
msgid "Creating Your First Stack"
msgstr ""

#: ../deployment/opsworks/installation.rst:4
msgid ""
"Setting up a Stack with all of its layers is a tedious excersise it TTW "
"configuration. Thankfully there's another AWS tool (there's always another "
"AWS tool) called CloudFormation that lets us quickly configure a basic Plone"
" stack with the most common layers configured."
msgstr ""

#: ../deployment/opsworks/installation.rst:9
msgid ""
"If you navigate to CloudFormation in the AWS console you'll be presented "
"with the option to create a ``Stack``. Confusingly, a CloudFormation Stack "
"is not the same thing as an Opsworks Stack, but the former is what we use to"
" automate the creation of the latter so we can use the terms a bit "
"interchangeably."
msgstr ""

#: ../deployment/opsworks/installation.rst:14
msgid ""
"You'll want to download the following file from Github: "
"https://raw.githubusercontent.com/alecpm/opsworks-web-"
"python/master/plone_buildout/examples/zeoserver-stack.template"
msgstr ""

#: ../deployment/opsworks/installation.rst:16
msgid ""
"And use the \"Upload a template to Amazon S3\" option to upload the above "
"file which provides a basic ZEO server stack configuration [*]_. You may "
"want to select the EC2 region for you stack before creating the stack, but "
"if you don't you can always clone the stack into another region later. The "
"stack creation will take a few minutes; once it succeeds you can navigate to"
" the Opsworks control panel to see your new Stack. [*]_"
msgstr ""

#: ../deployment/opsworks/installation.rst:23
msgid ""
"The CloudFormation setup creates a stack outside of a VPC (Virtual Private "
"Cloud), which is probably not ideal since some instance options are not "
"available outside of a VPC. If you want the stack to use a VPC or to be in a"
" different EC2 region than you initially ran CloudFormation from, then you "
"can clone the Stack from the Opsworks Dashboard to set your desired region "
"and VPC settings."
msgstr ""

#: ../deployment/opsworks/installation.rst:30
msgid ""
"There are a few important settings which CloudFormation is not able to "
"manage and have to be modified after stack creation. The two Apps (``Plone "
"Instances`` and ``Zeoserver``) should be edited to set the ``Data Source`` "
"to ``None`` (this setting is useful for a Relstorage configuration, but does"
" nothing for a ZEO server configuraiton). Eventually, you will probably want"
" to use your own buildout repository in these App configurations, but any "
"buildout you use should probably be cloned from the one used in this demo "
"configuration because it provides a number of buildout parts and variables "
"that the deployment recipes expect to be in place: "
"https://github.com/alecpm/opsworks_example_buildouts"
msgstr ""

#: ../deployment/opsworks/installation.rst:41
msgid ""
"Finally, you'll need to provide some additional configuration (Chef "
"Attributes) in the form of the Stack ``Custom JSON`` which can be edited in "
"the Stack Settings control panel. The following should be a reasonable "
"starting point::"
msgstr ""

#: ../deployment/opsworks/installation.rst:61
msgid ""
"Note the ``buildout_additional_config`` attribute, which allows you to "
"insert arbitrary configuration and overrides into the generated buildout "
"``deploy.cfg``. In this case, it's used to set a custom admin password for "
"your new plone instance. There are similar ``buildout_parts_to_include`` and"
" ``buildout_extends`` attributes which allow you to customize the parts used"
" for a particular deploy and any additional configuration files to include. "
"For example, typically I will use a include a ``cfg/production_sources.cfg``"
" in my production stack which sets revision/tag pins for any external source"
" dependencies in for production deployments."
msgstr ""

#: ../deployment/opsworks/installation.rst:71
msgid ""
"You may also wish to set the ``Opsworks Agent Version`` to ``Use latest "
"version``, and choose a ``Hostname Theme`` for fun."
msgstr ""

#: ../deployment/opsworks/installation.rst:74
msgid ""
"Note that this default configuration uses a blob directory shared over NFS. "
"That's not necessary if you're going to use a single intsance configuration "
"that you plan never to grow (perhaps for a staging server), but if you think"
" you might want multiple servers running ZEO clients, then starting out with"
" a network shared storage for blobs is probably the best way to go. You "
"could also configure shared blobs using the GlusterFS distributed filesystem"
" (this can be tricky and is only recommended if you are already familiar "
"with GlusterFS), S3-fuse Fs (slow), or serve them from the ZEO Server or "
"Relstorage DB. If you do want a single server configuration with no network "
"blob share, then you'll need to add a line of configuration for the blob "
"storage location, e.g.::"
msgstr ""

#: ../deployment/opsworks/installation.rst:90
msgid ""
"You may also want to change the load balancer stats access password in the "
"HAProxy layer."
msgstr ""

#: ../deployment/opsworks/installation.rst:93
msgid ""
"By default, each server is protected by a firewall that only allows access "
"to specific services defined by that instances layers. Our layers are "
"heavily customized, so the defaults aren't always sufficient. You'll want to"
" ensure that the servers can all communicate with one another over all "
"desired ports, and you'll probably want to be able to bypass the default "
"firewall from specific externalIP addresses to get direct access to your ZEO"
" Clients, etc. The simplest way to do that is to go to the Security tab for "
"each of the Layers and add the ``default`` security group to each of them. "
"[*]_"
msgstr ""

#: ../deployment/opsworks/installation.rst:103
msgid ""
"There is also a RelStorage version of this template, though turning a "
"Zeoserver Stack into a Relstorage Stack simply involves deleting the ZEO "
"server Layer and adding a built-in Memcached Layer."
msgstr ""

#: ../deployment/opsworks/installation.rst:105
msgid ""
"Before creating a CloudFormation Stack you'll be asked to confirm that AWS "
"resources may be created. The stack template here only creates cost-free "
"configuration resources."
msgstr ""

#: ../deployment/opsworks/installation.rst:107
msgid ""
"This could be done with more granularity, but ``default`` is usually a safe "
"bet. By default, ``default`` allows servers within your VPC full access to "
"one another, but doesn't permit any outside access. You can configure the "
"``default`` security group to allow your personal IPs direct access to any "
"specific ports you may want want to access remotely."
msgstr ""

#: ../deployment/opsworks/installation.rst:111
msgid "Adding an Instance"
msgstr ""

#: ../deployment/opsworks/installation.rst:117
msgid ""
"At this point you can navigate to the ``Instances`` control panel and create"
" an instance in a particular layer. Once you've defined your first instance "
"you can assign it to additional layers. Once you pick an appropriate "
"instance size (t2.micro is fine for playing around), you should be able to "
"use the instance defaults, though the initial EBS volume size is something "
"you may want to configure later if you don't intend to use separate mount "
"points for data storage."
msgstr ""

#: ../deployment/opsworks/installation.rst:125
msgid ""
"Once you've created the first instance you'll want to add it other layers "
"using the ``Existing Opsworks`` tabs. You will probably want to skip the "
"``EBS Snapshotting`` layer for now, and if you disabled NFS you should skip "
"the ``Shared Blobs`` layer too."
msgstr ""

#: ../deployment/opsworks/installation.rst:130
msgid ""
"By default the ``Zeoserver`` layer and the ``Shared Blobs`` layer both "
"create and attach EBS volumes to any instances assigned to them (for the "
"filestorage and NFS shared blobstorage respectively). This is optional when "
"using an EBS backed instance with an adequately sized root volume, but is "
"mandatory when using instance store backed instances. Traditionally, "
"instance store backed instances had some performance and cost advantages, "
"but those advantages have largely vanished recently, and EBS instances can "
"stop and start much faster after initial instance creation. For testing you "
"may want to delete the EBS volume resources from those layers before "
"starting your instance."
msgstr ""

#: ../deployment/opsworks/installation.rst:142
msgid ""
"I still like using instance store instances with sepearate attached EBS "
"volumes because those instances make no promises about retaining "
"configuration changes outside of the explicitly mounted EBS volumes, and "
"that keeps me from twiddling server configuration in ways that might not be "
"repeatable. They also help avoid some I/O concurrency issues you may run "
"into with an all EBS configuration, and allow more straighforward vertical "
"scaling."
msgstr ""

#: ../deployment/opsworks/installation.rst:150
msgid ""
"Now you should be able to start your instance, and after a little while "
"(depending on the instance size), you will have a server up and running."
msgstr ""

#: ../deployment/opsworks/installation.rst:153
msgid ""
"This Zope instance won't have a Plone site yet, so having added the "
"``default`` security group earlier in order to allow yourself direct access "
"to the ZEO clients will come in handy here. Your instance should have a "
"public IP address (the front end layer assigns an Elastic IP by default, "
"though you could manually transfer one in if you were moving an existing EC2"
" server). You should be able to access the first ZEO client at port 8081 and"
" create your Plone site."
msgstr ""

#: ../deployment/opsworks/installation.rst:163
msgid "Caveats"
msgstr ""

#: ../deployment/opsworks/installation.rst:165
msgid ""
"There are a few restrictions on what can and can't be done when in of "
"Opsworks which can occasionally cause annoyance:"
msgstr ""

#: ../deployment/opsworks/installation.rst:168
msgid ""
"Instances can only be added to layers when the Instances are stopped. So you"
" cannot add additional Layers of functionality to an already running "
"Instance. There are workarounds for this limitation (such as adding recipes "
"or package dependencies to existing layers and re-running the relevant "
"phases), but it can be frustrating."
msgstr ""

#: ../deployment/opsworks/installation.rst:170
msgid ""
"You cannot change the security groups of a running instance, and changes to "
"a Layer's security groups don't apply to running instances. Thankfully, any "
"changes to the firewall rules for a security group will affect all running "
"instances in that group. It's best to make sure your Layers assign all the "
"security groups you might need before starting an instance."
msgstr ""

#: ../deployment/opsworks/installation.rst:172
msgid ""
"A setup or deploy may fail because of problems accessing Repos or PyPI "
"packages. If the initial instance setup fails, it is not generally necessary"
" to stop, wait and then start the instance (which can take a long time), you"
" generally can re-run the ``setup`` phase from the Stack panel using the "
"``Run Command`` button."
msgstr ""

#: ../deployment/opsworks/installation.rst:174
msgid ""
"Downloading public packages from PyPI and dist.plone.org is often the "
"slowest part of initial instance setup. It can help tremendously to have a "
"tarball of all required eggs stored in a public S3 url, you can use the "
"Custom JSON to tell OpsWorks to fetch this tarball before running the "
"buildout. The configuration goes under the ``deploy[app_name]`` key and "
"looks like [*]_ ::"
msgstr ""

#: ../deployment/opsworks/installation.rst:178
msgid ""
"This configuration assumes that the tarball has top-level directory called "
"``eggs``. If you've setup such a tarball in an S3 bucket (usually creating "
"it from your first instance deploy), you simply add this configuration to "
"both the ``deploy[\"plone_instances\"]`` and ``deploy[\"zeoserver\"]`` "
"Custom JSON before launching an instance."
msgstr ""

#: ../deployment/opsworks/intro.rst:4
msgid ""
"The subject of this training is using Amazon Opsworks deployment system to "
"orchestrate complex, scalable, and redundant multi-server deployments of "
"Plone. The tools presented herein provide a mechanism for generically "
"defining server requirements and resources to launch fully configured Amazon"
" EC2 instances running Plone in a coordinated distributed manner."
msgstr ""

#: ../deployment/opsworks/intro.rst:10
msgid ""
"Amazon Opsworks does not provide the flexibility of Ansible deployments. It "
"is tied to Amazon cloud infrastructure, and is only fully tested for servers"
" running Ubuntu LTS. It does provide an unique infrastructure to automate "
"communication among multiple servers, allowing automated discovery and "
"inclusion of resources, and facilitating features like auto-scaling and "
"auto- healing."
msgstr ""

#: ../deployment/opsworks/intro.rst:17
msgid ""
"Opsworks is built on Chef, which is a configuration management system "
"similar to Ansible, but built on Ruby [*]_. The tools and concepts described"
" here attempt to ensure that you can deploy a complex Plone site without "
"having to learn any Chef or Ruby."
msgstr ""

#: ../deployment/opsworks/intro.rst:22
msgid "Yuck!"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:2
msgid "What Doesn't It Do"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:5
msgid "Storage Options"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:7
msgid ""
"Amazon recently introduced ``Elastic File System`` an effectively unlimited "
"size cloud file storage that can be mounted simultaneously on multiple "
"servers. It provides high availability and durability and should be "
"significantly faster than either S3 or even standard SSD EBS mounts. For "
"these reasons it would make an ideal storage option for a shared blob "
"directory and possibly also ZEO filestorages."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:14
msgid ""
"Integrating this new storage option into the recipes and documentation "
"should be a high priority going forward. The interface for Elastic File "
"System is NFS v4, which the stack already supports, so it may even be "
"trivial to integrate."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:18
msgid ""
"There are probably some other fun new AWS services that would be useful to "
"integrate."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:23
msgid "Proxy Cache Purging"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:25
msgid ""
"Plone provides some very nice proxy caching configuration, but that "
"configuration is managed TTW and stored persistently in the ZODB. If you "
"have multiple proxy caches which could be going online or offline "
"automatically or changing IP addresses, then having persistent configuration"
" of caches to purge is not ideal."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:31
msgid ""
"It would be very useful to add support in plone.app.caching for reading a "
"list of proxy servers from an environment variable or other mechanism that "
"can easily be managed as part of the configuration phase."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:37
msgid "Chef 12"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:39
msgid ""
"The latest Opsworks codebase requires Chef 12. The Python cookbooks are "
"currently only tested on Chef 11. Running Opsworks on Ubuntu Xenial "
"instances requires using the latest Chef 12 version. This will likely "
"require extensive testing and upgrades to dependency cookbooks."
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:46
msgid "Other Stuff?"
msgstr ""

#: ../deployment/opsworks/loose-ends.rst:48
msgid "Probably, play around with it and let me know."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:2
msgid "Maintenance"
msgstr "Mantenimiento"

#: ../deployment/opsworks/maintenance.rst:5
msgid "Backups"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:7
msgid ""
"The recipes automatically setup weekly ZODB packing and log rotation. I like"
" to Amazon's EBS snapshot feature for backups, and the EBS Snapshotting "
"layer provides that functionality automatically."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:11
msgid ""
"It requires you to use the AWS IAM Console to create a new user with the "
"following permissions::"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:19
msgid ""
"You will need to note the API credentials for this new user and enter them "
"into the Stack Custom JSON as follows::"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:28
msgid ""
"The EBS Snapshotting Layer should be assigned to any production instance "
"which has EBS volumes on which you are storing data. Generally speaking, any"
" production instance with the Zeoserver, Shared Blob, or Solr Layers "
"assigned should also have the EBS Snapshotting Layer assigned."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:33
msgid ""
"This Layer will setup automatic nightly snapshots of all mounted EBS "
"volumes. By default it retains up to 15 snapshots, but that can be "
"configured setting ``ebs_snapshots[\"keep\"]`` to the number you wish to "
"retain in the Stack Custom JSON."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:40
msgid "Updates"
msgstr "Actualizaciones"

#: ../deployment/opsworks/maintenance.rst:42
msgid ""
"Ubuntu security and OS package updates can be automated by adding the "
"following Custom JSON config::"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:56
msgid ""
"AWS provides various monitoring and alerting features, but most alerting "
"features need to be manually configured on a per EC2 instance basis. That's "
"not so convenient for a stack of instances which may grow, shrink or change "
"over time. For that reason I like to use New Relic for server monitoring. "
"There is built-in integration in the recipes, which includes detailed "
"performance server and client performance monitoring for Plone, as well as "
"plugins for Nginx, Varnish and HAProxy services and standard CPU, Disk space"
" and RAM server metrics."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:65
msgid ""
"There's also a recipe provided to integrate the Papertrail log tracing and "
"searching service. To help you live the dream of never having to SSH into "
"your servers."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:71
msgid "Sending Mail"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:73
msgid ""
"It's possible, and not difficult to install and configure a mailer using a "
"chef postfix recipe and  some more Custom JSON. However, I do not recommend "
"doing so. Cloud Servers generally, and EC2 specifically tend to land on SPAM"
" blacklists, ensuring your outgoing mail is not blackholed generally "
"requires some special care and requests to Amazon to setup reverse DNS and "
"whitelist any outgoing mail servers."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:80
msgid ""
"Instead I recommend using a hosted mail delivery service like Amazon SES or "
"perhaps GMail."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:85
msgid "SSH Access"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:87
msgid ""
"Ideally, you never have to login to your cloud server, but things go wrong "
"and you might have to eventually, even if only out of curiosity. [*]_ By "
"default OpsWorks does not assign an SSH key to new instances, but you can "
"set one if desired at either the instance or the Stack level. Better yet, "
"Opsworks allows more granular access control in combination with IAM. If you"
" create a user via the AWS IAM console (no permissions need be assigned, and"
" no credentials added or recorded for SSH access), you can then import that "
"user into the OpsWorks Users control panel. In OpsWorks users can be given "
"access to specific stacks, allowing them to view, deploy or manage them, as "
"well as granting them SSH and/or sudo accees to Stack Instances using a "
"public key that can be added through the web interface. Once you've imported"
" an IAM user into Opsworks and granted it SSH access with a public key, that"
" user should be able to log in to all instances in the stack. [*]_"
msgstr ""

#: ../deployment/opsworks/maintenance.rst:103
msgid ""
"A note on OS permissions: all application related files live under "
"``/srv/www`` and are generally owned by the ``deploy`` user with fairly "
"restricted permissions. Any user SSH'ing in will probably need to sudo to "
"the ``deploy`` user to see or do much of interest."
msgstr ""

#: ../deployment/opsworks/maintenance.rst:108
msgid ""
"You should *never* manually modify any configuration on a cloud configured "
"server, except for purposes of troubleshooting. Any changes you make to the "
"server should be made via the Stack configuration (i.e. the Custom JSON and "
"the Recipes assigned to Layers)."
msgstr ""

#: ../deployment/opsworks/running.rst:2
msgid "Deploying Changes"
msgstr ""

#: ../deployment/opsworks/running.rst:4
msgid ""
"Now that you've got one or more Instances up and running, you may need to "
"update the code on them. Traditionally, you'd SSH into the server pull in "
"new changes from the repo(s), run buildout, and restart ZEO clients if "
"necessary. With Opsworks, you click a deploy button and everything is "
"handled automtically."
msgstr ""

#: ../deployment/opsworks/running.rst:10
msgid "So what happens when you click the Deploy button for an Opsworks App?"
msgstr ""

#: ../deployment/opsworks/running.rst:12
msgid ""
"The instance looks to see if there's a new revision on the App's buildout "
"repository (accounting for the branch or revision setting in the App "
"configuration)."
msgstr ""

#: ../deployment/opsworks/running.rst:14
msgid ""
"If there are changes to the repository, then it makes a new clone of the "
"repository and puts it in a directory under ``releases`` named by the "
"checkout timestamp. It then generates a new ``deploy.cfg`` based on the "
"Stack Configuration, including information about currently running instances"
" and layers. Then it runs bootstrap and buildout with that configuration. If"
" the process succeeds, it symlinks the ``release/$timestamp`` directory to "
"``current`` and restarts the ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:16
msgid ""
"If there are no changes to the repository, then it generates a new "
"``deploy.cfg`` based on the current Stack configuration. If that file "
"differs from the existing deploy.cfg (e.g. because of changes in the Stack's"
" Custom JSON), then it will re-run buildout, and - if the buildout succeeds "
"- restart the ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:18
msgid ""
"If there are no changes to the repository, and the new ``deploy.cfg`` is "
"identical to the prior version, then it checks for an "
"``always_buildout_on_deploy`` flag in the Stack's ``deploy[appname]`` Custom"
" JSON. If that flag is true, then it runs buildout and restarts the ZEO "
"clients on success. This is useful if you are deploying changes from "
"external repositories pulled in by mr.developer, even when the buildout repo"
" itself hasn't changed."
msgstr ""

#: ../deployment/opsworks/running.rst:20
msgid ""
"Steps 1 and 2 are essentially a ``Capistrano`` style deployment familar from"
" the Rails world. This process allows for explicit rollback of deployed code"
" to prior versions at any time. Steps 3 and 4 are buildout specific and "
"don't support rollbacks in the same way."
msgstr ""

#: ../deployment/opsworks/running.rst:25
msgid ""
"You can run a deploy on a single Instance or on many at once. The deploy "
"will run in parallel on all Instances selected. Instances that have the "
"deployed App/Layer assigned will go through the process above, other "
"instances will run a generic deploy phase (which allows those Instances to "
"update their configuration in parallel). This process creates a good chance "
"that all your ZEO clients will be restarted at once, causing a temporary "
"outage and a slow site. If you have multiple Instances running ZEO clients "
"you can deploy to them one at a time, to avoid an outage. You can also "
"configure your Stack to do rolling deploys by adding a ``restart_delay`` in "
"seconds to your Custom JSON under the top-level ``plone_instances`` key. "
"When that is set, the deploy will wait that amount of time between each ZEO "
"client restart for a given Instance."
msgstr ""

#: ../deployment/opsworks/running.rst:38
msgid ""
"If you use Travis CI to provide automatic testing of your "
"buildout/application, you can configure Travis to automatically lanuch an "
"Opsworks deploy for a specific Stack and Application on successful builds "
"(see https://docs.travis-ci.com/user/deployment/opsworks/)."
msgstr ""

#: ../deployment/opsworks/running.rst:45
msgid "Instance Sizes"
msgstr ""

#: ../deployment/opsworks/running.rst:47
msgid ""
"There are many available Instance types on EC2, which makes choosing the "
"correct Instance sizes for your application cluster a bit of an art.  The "
"Opsworks recipes will automatically factor in the CPU capacity of the "
"Instances you choose for your ZEO client Layers (using their Elastic Compute"
" Unit/Core counts) to determine automatically how many ZEO clients to create"
" per Instance. You can fine tune that calculation further by setting the "
"``per_cpu`` attribute under the ``plone_instances`` key in the Stack Custom "
"JSON. You can also tweak the ``zodb_cache_size``, and ``zserver_threads`` to"
" help tune RAM usage for your ZEO clients."
msgstr ""

#: ../deployment/opsworks/running.rst:59
msgid "Scaling"
msgstr ""

#: ../deployment/opsworks/running.rst:61
msgid ""
"If you've setup a distributed blob storage (whether with NFS/GlusterFS, "
"S3FS, ZEO or Relstorage), adding more ZEO clients is a simple matter of "
"defining a new instance assigned only to your Plone Instances application "
"Layer and starting it."
msgstr ""

#: ../deployment/opsworks/running.rst:66
msgid ""
"In addition to the normal 24/7 instances, you can define time-based "
"Instances that automatically add instances during regular peak traffic "
"periods."
msgstr ""

#: ../deployment/opsworks/running.rst:72
msgid ""
"Alternatively, you can define load-based instances which automatically start"
" up and shutdown based on the average CPU usage, Load, or RAM usage of "
"existing instances in the layer."
msgstr ""

#: ../deployment/opsworks/running.rst:79
msgid ""
"Any new instances will automatically discover your existing ZEO server. Any "
"load balancers will automatically discover any new ZEO clients. The Stack "
"reconfiguration will happen automatically whenever an instance goes up or "
"down."
msgstr ""

#: ../deployment/opsworks/running.rst:84
msgid ""
"You can view the HAProxy ZEO client status by visiting the password "
"protected url ``/balancer/stats`` for your front end instance IP(s)."
msgstr ""

#: ../deployment/opsworks/running.rst:87
msgid ""
"For a high traffic site that requires a high availability configuration, it "
"may also make sense to run the front end HAProxy layer on multiple Instances"
" in different Availablilty Zones. You would need to route external traffic "
"to those servers using an adaptive DNS service or Amazon's Elastic Load "
"Balancer."
msgstr ""

#: ../deployment/opsworks/running.rst:94
msgid "Configuration"
msgstr ""

#: ../deployment/opsworks/running.rst:96
msgid ""
"The Stack Custom JSON configuration offers a number of entry points for "
"customizing the default Stack without needing to learn any Chef or Ruby. "
"Those configuration parameters are thoroughly documented in the Plone "
"Buildout cookbook `README`_, and the full list of Plone Buildout cookbook "
"specific attributes is in `attributes/default.rb`_. Any of those attributes "
"can be customized via the Stack Custom JSON. For example, the "
"``nginx_plone[\"additional_configuration\"]`` and "
"``nginx_plone[\"additional_servers\"]`` may be the most generically useful "
"items for front end configuration."
msgstr ""

#: ../deployment/opsworks/running.rst:106
msgid ""
"The recipes and example buildout also include optional support for running "
"and configuring a Solr search server and setting up a Celery task queue for "
"running asynchronous jobs using collective.celery."
msgstr ""

#: ../deployment/opsworks/terms.rst:2
msgid "Deployment Terminology"
msgstr ""

#: ../deployment/opsworks/terms.rst:4
msgid ""
"It's probably a good idea to be familar with a few core Chef concepts, "
"though digging deeply into Chef is definitely not something I encourage "
"Python developers to do."
msgstr ""

#: ../deployment/opsworks/terms.rst:9
msgid ""
"``Resource``: The basic building block in Chef (and also Ansible); defines "
"files, directories, installed packaes, services, etc."
msgstr ""

#: ../deployment/opsworks/terms.rst:11
msgid ""
"``Recipe``: A collection of resource definitions with some logic to connect "
"them. These can be very simple or extraordinarily complex; A recipe can "
"depend on other recipes. These basically play the same role as ``Tasks`` in "
"Ansible."
msgstr ""

#: ../deployment/opsworks/terms.rst:13
msgid ""
"``Cookbook``: A collection of recipes required to setup a service or "
"similar. These play a similar role to ``Roles`` in Ansible. These generally "
"can be found in the Chef Supermarket like Roles from the Ansible Galaxy."
msgstr ""

#: ../deployment/opsworks/terms.rst:15
msgid ""
"``Berkshelf``: A single file configuration defining the set of cookbooks "
"needed for a deployment. It consists of a ``Berksfile`` which defines "
"locations and versions of all cookbooks required for a deployment."
msgstr ""

#: ../deployment/opsworks/terms.rst:17
msgid ""
"``Attributes``: The deployment specific configuration for the cookbook and "
"recipes. This is essentially a collection of JSON like primitives, similar "
"to YAML group/host ``Vars``."
msgstr ""

#: ../deployment/opsworks/terms.rst:21
msgid "Opsworks"
msgstr ""

#: ../deployment/opsworks/terms.rst:23
msgid ""
"Amazon Opsworks takes this basic configuration framework and provides its "
"own set of concepts, to implement cluster orchestration. When using "
"Opsworks, you will be making use built-in Opsworks Chef Cookbooks provided "
"by Amazon. These built-in Cookbooks provide a number of Recipes for "
"configuring and deploying many types of applications using simple TTW "
"configuration from the Opsworks control panel. These include Node.js, Rails,"
" PHP, and Java applications, but not Python [*]_."
msgstr ""

#: ../deployment/opsworks/terms.rst:31
msgid ""
"I've created a couple supplemental Cookbooks that extend the existing "
"Opsworks deployment recipes to support Python and Plone along with other "
"supporting services that are useful when making production deployments of "
"Plone."
msgstr ""

#: ../deployment/opsworks/terms.rst:35
msgid ""
"Opsworks has its own vocabulary of concepts related to deploying and "
"orchestrating clusters of servers. The building blocks of Opsworks are:"
msgstr ""

#: ../deployment/opsworks/terms.rst:37
msgid ""
"``Stack``: The fundamental container for your configuration, this lives in a"
" particular EC2 region and contains all the configuration for your cluster. "
"Typically you would have a separate production stack and development stack. "
"Creating this is the first step in the process of defining your cluster. "
"Stacks can be cloned to replicate configuration across regions."
msgstr ""

#: ../deployment/opsworks/terms.rst:39
msgid ""
"``Layers``: A Layer defines a discrete set of functionality that may be "
"provided by a server Instance.  For example, a Plone cluster may have a "
"front end Layer running an Nginx web server, Varnish proxy cache and HAProxy"
" load balancer [*]_ , an Application Layer for your ZEO client instances, an"
" Application Layer for your ZEO server, and a maintenance layer to manage "
"database backups and packs. Layers define what recipes will be run on an "
"instance, and which OS packages it requires, along with any Amazon resources"
" and permssions are required to provide a service (e.g. static IP addresses,"
" additional EBS storage volumes)."
msgstr ""

#: ../deployment/opsworks/terms.rst:41
msgid ""
"``Instances``: An OpsWorks Instance is similar to an EC2 instance, it has a "
"type (e.g. from micro to xlarge), an OS and an Availability Zone, but it is "
"an abstraction. It becomes an actual EC2 instance once it's been started, "
"but before that it's simply a metadata about a desired server. Instances are"
" assigned to one or more Layers, and come in three varieties, 24/7, time-"
"based and load-based."
msgstr ""

#: ../deployment/opsworks/terms.rst:43
msgid ""
"``Apps``: An App points to a code repository (in our case a buildout) which "
"you want to deploy to a specific Application Layer. Typically you would have"
" an App for your Plone instances and another for your Zeoserver. Both these "
"Apps would typically point to the same buildout repository. You might also "
"create an App to configure a Plone specific Solr server or to run a "
"additional applications within the cluster."
msgstr ""

#: ../deployment/opsworks/terms.rst:45
msgid ""
"``Resources``: A set of Amazon EC2 resources that will be used by the stack "
"by being attached/assigned to Instances when they are started. These include"
" Elastic IP addresses, EBS storage volumes and RDS databases (useful you are"
" running Relstorage)."
msgstr ""

#: ../deployment/opsworks/terms.rst:47
msgid ""
"A Stack can be configured with a single Instance running all the Layers, or "
"multiple Instances each running different Layers. You might, for example, "
"have a production stack with five Instances running the Plone ZEO client "
"Application Layer, a single instance running the ZEO server Application "
"Layer, and two Instances running the front end proxy/loadbalancer Layer "
"(with an Elastic Load Balancer in front of those). You might also have a "
"staging stack with all the same Layers applied to a single modest server. "
"Other than the Instance definitions (and perhaps the App repository branch),"
" these Stacks would be essentially identical."
msgstr ""

#: ../deployment/opsworks/terms.rst:57
msgid "Boo!"
msgstr ""

#: ../deployment/opsworks/terms.rst:58
msgid ""
"Though you could separate each of these front end services into their own "
"layers if you really wanted to, we combine them by default under a "
"customized HAProxy layer which already provides a nice UI for a few HAProxy "
"features."
msgstr ""

#: ../deployment/opsworks/terms.rst:61
msgid "Instance Lifecycle"
msgstr ""

#: ../deployment/opsworks/terms.rst:63
msgid "Each Opsworks Instance goes through a few phases during its lifecycle:"
msgstr ""

#: ../deployment/opsworks/terms.rst:65
msgid "``setup``"
msgstr ""

#: ../deployment/opsworks/terms.rst:66
msgid "``deploy``"
msgstr ""

#: ../deployment/opsworks/terms.rst:67
msgid "``configure``"
msgstr ""

#: ../deployment/opsworks/terms.rst:68
msgid "``undeploy``"
msgstr ""

#: ../deployment/opsworks/terms.rst:69
msgid "``shutdown``"
msgstr ""

#: ../deployment/opsworks/terms.rst:71
msgid ""
"Each of these lifecycle phases runs recipes assigned to that phase in the "
"assigned Layers.  When these recipes are run, the Stack configuration is "
"passed to the server. This configuration includes complete information about"
" the state of the entire Stack and all of its running Instances."
msgstr ""

#: ../deployment/opsworks/terms.rst:76
msgid ""
"When an Instance starts, it first goes through a ``setup`` phase: installing"
" all package dependencies for all assigned Layers and running all the "
"recipes assigned to the ``setup`` phase of those Layers."
msgstr ""

#: ../deployment/opsworks/terms.rst:80
msgid ""
"Once the ``setup`` phase is complete, a ``deploy`` phase is automatically "
"started. running all the recipes assigned to the ``deploy`` phase of any "
"assigned Layers. Subsequently, you may manually run a ``deploy`` for a "
"specific Application on any or all of the instances to update the "
"application code and reconfigure services."
msgstr ""

#: ../deployment/opsworks/terms.rst:86
msgid ""
"The ``shutdown`` phase is run automatically before an instance is stopped."
msgstr ""

#: ../deployment/opsworks/terms.rst:88
msgid ""
"The ``undeploy`` phase is rarely used. It is triggered when an application "
"is manually removed from an instance."
msgstr ""

#: ../deployment/opsworks/terms.rst:91
msgid ""
"Whenever an Instance is started or stopped and it's ``setup`` or "
"``shutdown`` phase has completed a ``configure`` phase is initiated on all "
"running instances. As with all recipe runs, the ``configure`` phase recipes "
"are passed data about all the curently running Instances and their Layers so"
" that they can automatcially reconfigure themselves based on the updated "
"state of the Stack. For example, a load balancer may need to automatically "
"add or remove Plone ZEO clients from it's list of active backends, a ZEO "
"client may need to change its ZEO server or its Relstorage Memcache if "
"configuration for those services have changed."
msgstr ""

#: ../deployment/opsworks/terms.rst:101
msgid ""
"This ``configure`` phase, during which the current cluster state is "
"automatically shared with all the instances, is where the orchestration "
"magic happens."
msgstr ""

#: ../deployment/playbook_use.rst:3
msgid "Basic use of the playbook"
msgstr ""

#: ../deployment/playbook_use.rst:6
msgid "Local configuration file"
msgstr ""

#: ../deployment/playbook_use.rst:8
msgid ""
"For a quick start, copy one of the :file:`sample-*.yml` files to :file"
":`local-configure.yml`. The :file:`local-configure.yml` file is "
"automatically included in the main playbook if it's found."
msgstr ""

#: ../deployment/playbook_use.rst:15
msgid ""
"Now, edit the :file:`local-configure.yml` file to set some required "
"variables:"
msgstr ""

#: ../deployment/playbook_use.rst:17
msgid "admin_email"
msgstr ""

#: ../deployment/playbook_use.rst:19
msgid ""
"The server admin's email. Probably yours. This email address will receive "
"system notices and log analysis messages."
msgstr ""

#: ../deployment/playbook_use.rst:23
msgid "plone_initial_password"
msgstr ""

#: ../deployment/playbook_use.rst:25
msgid ""
"The initial administrative password for the Zope/Plone installation. Not the"
" same as the server shell login."
msgstr ""

#: ../deployment/playbook_use.rst:28
msgid "muninnode_query_ips"
msgstr ""

#: ../deployment/playbook_use.rst:30
msgid ""
"Are you going to run a Munin monitor on a separate machine? (And, if not, "
"why not?) Specify the IP address of the monitor machine. Or ..."
msgstr ""

#: ../deployment/playbook_use.rst:35
msgid "install_muninnode"
msgstr ""

#: ../deployment/playbook_use.rst:37
msgid ""
"Remove the \"#\" on the ``install_muninnode: no`` line if you are not using "
"a Munin monitor."
msgstr ""

#: ../deployment/playbook_use.rst:39
msgid ""
"You're also nearly certainly going to want to specify a Plone version via "
"the ``plone_version`` setting. You should be able to pick any version from "
"4.3.x or 5.x.x. Note that the value for this variable must be quoted to make"
" sure it's interpreted as a string."
msgstr ""

#: ../deployment/playbook_use.rst:44
msgid "Use with Vagrant"
msgstr ""

#: ../deployment/playbook_use.rst:46
msgid ""
"If you've installed Vagrant/Virtualbox, you're ready to test. Since Vagrant "
"manages the connection, you don't need to create a inventory file entry."
msgstr ""

#: ../deployment/playbook_use.rst:49
msgid ""
"There is a Vagrant setup file, :file:`Vagrantfile`, included with the "
"playbook, so you may just open a command-line prompt, make sure your Ansible"
" virtualenv is activated, and type:"
msgstr ""

#: ../deployment/playbook_use.rst:57
msgid ""
"The first time you use a \"box\" it will be downloaded. These are large "
"downloads; expect it to take some time."
msgstr ""

#: ../deployment/playbook_use.rst:62
msgid ""
"Instructor note: Having several students simultaneously downloading a "
"virtualbox over wifi or a slow connection is a nightmare. Have a plan."
msgstr ""

#: ../deployment/playbook_use.rst:66
msgid ""
"Once you've run :program:`vagrant up`, running it again will not "
"automatically provision the virtualbox. In this case, that means that "
"Ansible is not run. So, if you change your Ansible configuration, you'll "
"need to use:"
msgstr ""

#: ../deployment/playbook_use.rst:76
msgid ""
"When you run ``up`` or ``provision``, watch to make sure it completes "
"successfully. Note that failures for particular plays do not mean that "
"Ansible provisioning failed. The playbook has some tests that fail if "
"particular system features are unavailable. Those test failures are ignored "
"and the provisioning continues. The provisioning has only failed if an error"
" causes it to stop."
msgstr ""

#: ../deployment/playbook_use.rst:82
msgid "An example of an ignored failure::"
msgstr ""

#: ../deployment/playbook_use.rst:90
msgid "Vagrant ports"
msgstr ""

#: ../deployment/playbook_use.rst:92
msgid ""
"The Vagrant setup (in :file:`Vagrantfile`) maps several ports on the guest "
"machine (the virtualbox) to the host box. The general scheme is to forward a"
" host port that is 1000 greater than the guest port. For example, the load-"
"balancer monitor port on the guest server is ``1080``. On the host machine, "
"that's mapped by ssh tunnel to 2080. So, we may see the haproxy monitor at "
"``http://localhost:2080/admin``."
msgstr ""

#: ../deployment/playbook_use.rst:98
msgid ""
"The guest's http port (80) is reached via the host machine's port 1080 -- "
"but that isn't actually very useful due to URL rewriting for virtual "
"hosting. If you take a look at ``http://localhost:1080`` from your host "
"machine, you'll see the default Plone site, but stylesheets, javascript and "
"images will all be missing. Instead, look at the load-balancer port (8080 on"
" the guest, 9080 on the host) to see your ZODB root."
msgstr ""

#: ../deployment/playbook_use.rst:104
msgid "Some quick Vagrant"
msgstr ""

#: ../deployment/playbook_use.rst:115
msgid ""
"To each of the these commands, you may add an id to pick one of the boxes "
"defined in Vagrantfile. Read Vagrantfile for the ids. For example, "
"``centos7`` is the id for a CentOS box."
msgstr ""

#: ../deployment/playbook_use.rst:124
msgid "Run against cloud"
msgstr ""

#: ../deployment/playbook_use.rst:126
msgid ""
"Let's provision a cloud server. Here are the facts we need to know about our"
" cloud server:"
msgstr ""

#: ../deployment/playbook_use.rst:129
msgid "hostname"
msgstr ""

#: ../deployment/playbook_use.rst:131
msgid ""
"A new server may or may not have a DNS host entry. If it does, use that "
"hostname. If not, invent one and be prepared to supply an IP address."
msgstr ""

#: ../deployment/playbook_use.rst:135
msgid "login id"
msgstr ""

#: ../deployment/playbook_use.rst:137
msgid ""
"The user id of a system account that is either the superuser (root) or is "
"allowed to use :command:`sudo` to issue arbitrary commands as the superuser."
msgstr ""

#: ../deployment/playbook_use.rst:139
msgid "password"
msgstr ""

#: ../deployment/playbook_use.rst:141
msgid ""
"If your cloud-hosting company does not set up the user account for ssh-"
"keypair authentication, you'll need a password. Even if your account does "
"allow passwordless login, it may still require a password to run "
":command:`sudo`."
msgstr ""

#: ../deployment/playbook_use.rst:144
msgid ""
"If your cloud-hosting company sets up a root user and password, it's a good "
"practice to login (or use Ansible) to create a new, unprivileged user with "
"sudo rights. Cautious sysadmins will also disable root login via ssh."
msgstr ""

#: ../deployment/playbook_use.rst:147
msgid "connection details"
msgstr ""

#: ../deployment/playbook_use.rst:149
msgid ""
"If you don't have a DNS host record for your server, you'll need to have its"
" IP address. If ssh is switched to an alternate port, you'll need that port "
"number."
msgstr ""

#: ../deployment/playbook_use.rst:152
msgid ""
"With that information, create an inventory file (if none exists) and create "
"a host entry in it. We use :file:`inventory.cfg` for an inventory file. A "
"typical inventory file::"
msgstr ""

#: ../deployment/playbook_use.rst:158
msgid ""
"You may leave off the ``ansible_host`` if the name supplied matches the DNS "
"host record. You may leave off the ``ansible_user`` if your user id is the "
"same on the server."
msgstr ""

#: ../deployment/playbook_use.rst:161
msgid ""
"An inventory file may have many entries. You may run Ansible against one, "
"two, all of the hosts in the inventory file, or against alias groups like "
"\"plone-servers\". See `Ansible's inventory documentation "
"<http://docs.ansible.com/ansible/intro_inventory.html>`_ for information on "
"grouping host entries and for more specialized host settings."
msgstr ""

#: ../deployment/playbook_use.rst:165
msgid ""
"Now, let's make things easier for us going forward by creating an "
":file:`ansible.cfg` file in our playbook directory. In that text file, "
"specify the location of your inventory file:"
msgstr ""

#: ../deployment/playbook_use.rst:175
msgid "Smoke test"
msgstr ""

#: ../deployment/playbook_use.rst:177
msgid ""
"Now, let's see if we can use Ansible to connect to the remote machine that "
"we've specified in our inventory."
msgstr ""

#: ../deployment/playbook_use.rst:179
msgid ""
"Does the new machine allow an ssh key login, then you ought to be able to "
"use the command:"
msgstr ""

#: ../deployment/playbook_use.rst:185
msgid "If you need a password for login, try:"
msgstr ""

#: ../deployment/playbook_use.rst:191
msgid "And, if that fails, ask for verbose feedback from Ansible:"
msgstr ""

#: ../deployment/playbook_use.rst:197
msgid ""
"Now, let's test our ability to become superuser on the remote machine. If "
"you have passwordless sudo, this should work:"
msgstr ""

#: ../deployment/playbook_use.rst:205
msgid "If sudo requires a password, try:"
msgstr ""

#: ../deployment/playbook_use.rst:212
msgid ""
"If all that works, congratulations, you're ready to use Ansible to provision"
" the remote machine."
msgstr ""

#: ../deployment/playbook_use.rst:216
msgid ""
"The \"become\" flag tells Ansible to carry out the action while becoming "
"another user on the remote machine. If no user is specified, we become the "
"superuser. If no method is specified, it's done via :command:`sudo`."
msgstr ""

#: ../deployment/playbook_use.rst:220
msgid ""
"You won't often use the ``--become`` flag because the playbooks that need it"
" specify it themselves."
msgstr ""

#: ../deployment/playbook_use.rst:223
msgid "Diagnosing ssh connection failures"
msgstr ""

#: ../deployment/playbook_use.rst:225
msgid ""
"If Ansible has trouble connecting to the remote host, you're going to get a "
"message like:"
msgstr ""

#: ../deployment/playbook_use.rst:235
msgid ""
"If this happens to you, try adding ``-vvv`` to the :program:`ansible` or "
":program:`ansible-playbook` command line. The extra information may -- or "
"may not -- be useful."
msgstr ""

#: ../deployment/playbook_use.rst:238
msgid ""
"The real test is to use a direct ssh login in order to get the ssh error. "
"There's a pretty good chance that the identity of the remote host will have "
"changed, and ssh will give you a command line to clean it up."
msgstr ""

#: ../deployment/playbook_use.rst:242
msgid "Running the playbook"
msgstr ""

#: ../deployment/playbook_use.rst:244
msgid ""
"We're ready to run the playbook. Make sure you're logged to your ansible-"
"playbook directory and that you've activated the Python virtualenv that "
"includes Ansible."
msgstr ""

#: ../deployment/playbook_use.rst:247
msgid ""
"If you're targetting all the hosts in your inventory, running the playbook "
"may be as easy as:"
msgstr ""

#: ../deployment/playbook_use.rst:253
msgid "If you need a password for ssh login, add ``-k``."
msgstr ""

#: ../deployment/playbook_use.rst:255
msgid "If you need a password for sudo, add ``-K``."
msgstr ""

#: ../deployment/playbook_use.rst:257
msgid "If you need a password for both, add \"-k -K\"."
msgstr ""

#: ../deployment/playbook_use.rst:259
msgid ""
"If you want to target a particular host in your inventory, add "
"``--limit=hostname``. Note that the ``--limit`` parameter is a search term; "
"all hostnames matching the parameter will run."
msgstr ""

#: ../deployment/playbook_use.rst:263
msgid ""
"As with Vagrant, check the last message to make sure it completes "
"successfully. When first provisioning a server, timeout errors are more "
"likely. If you have a timeout, just run the playbook again. Note that "
"failures for particular plays do not mean that Ansible provisioning failed."
msgstr ""

#: ../deployment/playbook_use.rst:269
msgid "Firewalling"
msgstr ""

#: ../deployment/playbook_use.rst:271
msgid ""
"Running the Plone playbook does not set up server firewalling. That's "
"handled via a separate playbook, included with the kit. We've separated the "
"functions because many sysadmins will wish to handle firewalling themselves."
msgstr ""

#: ../deployment/playbook_use.rst:275
msgid "If you wish to use our firewall playbook, just use the command:"
msgstr ""

#: ../deployment/playbook_use.rst:281
msgid ""
":file:`firewall.yml` is just a dispatcher. Actual firewall code is in the "
":file:`firewalls` subdirectory and is platform-specific. ``ufw`` is used for"
" the Debian-family; ``firewalld``"
msgstr ""

#: ../deployment/playbook_use.rst:285
msgid ""
"The general firewall strategy is to block everything but the ports for ssh, "
"http, https and munin-node. The munin-node port is restricted to the monitor"
" IP you specify."
msgstr ""

#: ../deployment/playbook_use.rst:290
msgid ""
"This strategy assumes that you're going to use ssh tunnelling if you need to"
" connect to other ports."
msgstr ""

#: ../deployment/plone_playbook.rst:3
msgid "The Plone Playbook"
msgstr ""

#: ../deployment/plone_playbook.rst:6
msgid "Currently supported platforms"
msgstr ""

#: ../deployment/plone_playbook.rst:8
msgid ""
"We currently support two Linux families: Debian and RHEL. *Support* means "
"that the playbook knows how to load platform package dependencies and how to"
" set up users, groups, and the platform's method for setting up daemons to "
"start and stop with the operating system."
msgstr ""

#: ../deployment/plone_playbook.rst:13
msgid ""
"There's no particular reason why we can't extend that support to other "
"families, like BSD. All we need is a champion to take responsibility for "
"extending and testing on other platforms."
msgstr ""

#: ../deployment/plone_playbook.rst:16
msgid "Debian"
msgstr ""

#: ../deployment/plone_playbook.rst:18
msgid ""
"Our goal is to support the current Ubuntu LTS and the Debian equivalent. "
"Currently we're doing a bit better than that. On Ubuntu we're supporting "
"everything from Trusty to Xenial. On Debian, we're working with both Jessie "
"and Wheezy."
msgstr ""

#: ../deployment/plone_playbook.rst:23
msgid "RHEL"
msgstr ""

#: ../deployment/plone_playbook.rst:25
msgid ""
"We're currently only testing on CentOS 7. If you're using Plone on RHEL, we "
"could use your help on extending that support."
msgstr ""

#: ../deployment/plone_playbook.rst:29
msgid "Quick review of contents"
msgstr ""

#: ../deployment/plone_playbook.rst:31
msgid ""
"Let's quickly review what you're getting when you check out the Plone "
"Ansible Playbook."
msgstr ""

#: ../deployment/plone_playbook.rst:36
msgid "We include two playbooks:"
msgstr ""

#: ../deployment/plone_playbook.rst:38
msgid "playbook.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:40
msgid "The main playbook that sets everything except the firewall."
msgstr ""

#: ../deployment/plone_playbook.rst:42
msgid "firewall.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:44
msgid ""
"A separate playbook to set up the software firewall. Most sysadmins have "
"their own firewall experience, and may or may not choose to use this "
"playbook."
msgstr ""

#: ../deployment/plone_playbook.rst:48
msgid "roles"
msgstr "roles"

#: ../deployment/plone_playbook.rst:50
msgid ""
"Roles are basically pre-packaged subroutines with their own default "
"variables. Several roles are part of the Plone Ansible Playbook kit and will"
" be present in your initial checkout. These include roles that set up the "
"haproxy load balancer, varnish cache, nginx http server, postfix SMTP agent,"
" munin-node monitoring, logwatch log analysis, message-of-the-day and a "
"fancy setup for restarting ZEO clients."
msgstr ""

#: ../deployment/plone_playbook.rst:54
msgid ""
"Other roles, including the role that actually sets up Plone, are loaded when"
" you use ``ansible-galaxy`` to fetch the items listed in "
":file:`requirements.yml`. Except for the Plone server role, these are "
"generally very generic Ansible Galaxy roles that we liked."
msgstr ""

#: ../deployment/plone_playbook.rst:58
msgid "Vagrant"
msgstr "Vagrant"

#: ../deployment/plone_playbook.rst:60
msgid ""
"Vagrant/Virtualbox is a very handy way to test your playbook, both during "
"development and for future maintenance. We include a couple of files to help"
" you get started with Vagrant testing."
msgstr ""

#: ../deployment/plone_playbook.rst:63
msgid "Vagrantfile"
msgstr ""

#: ../deployment/plone_playbook.rst:65
msgid ""
"A Vagrant setup file that will allow you to create guest virtual hosts for "
"any of the platforms we support and will run Ansible as the provisioner with"
" playbook.yml. This currently defaults to building a Trusty box, but you may"
" pick others by naming them on the :command:`vagrant up` command line."
msgstr ""

#: ../deployment/plone_playbook.rst:68
msgid "vbox_host.cfg"
msgstr ""

#: ../deployment/plone_playbook.rst:70
msgid ""
"When you use vagrant commands, vagrant controls the ssh connection. "
":file:`vbox_host.cfg` is an Ansible inventory file that should allow you to "
"run your playbook directly (without the :command:`vagrant` command) against "
"your guest box."
msgstr ""

#: ../deployment/plone_playbook.rst:74
msgid "Sample configurations"
msgstr ""

#: ../deployment/plone_playbook.rst:76
msgid "The playbook kit contains several sample configuration files."
msgstr ""

#: ../deployment/plone_playbook.rst:78
msgid "sample-very-small.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:80
msgid ""
"Targets a server with 512MB of memory and one CPU core. Sets up one ZEO "
"client with two threads with very small object caches. No load balancer. "
"Varnish cache is file-based."
msgstr ""

#: ../deployment/plone_playbook.rst:85
msgid "sample-small.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:87
msgid ""
"Targets a server with 1GB of memory and one CPU core. Sets up one ZEO client"
" with two threads with small object caches. No load balancer. Varnish cache "
"is file-based."
msgstr ""

#: ../deployment/plone_playbook.rst:92
msgid "sample-medium.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:94
msgid ""
"Targets a server with 2GB of memory and two CPU cores. Sets up two ZEO "
"clients, each with one thread with a medium object cache. Uses load balancer"
" to manage the queue to the ZEO clients. Varnish cache is memory-based."
msgstr ""

#: ../deployment/plone_playbook.rst:99
msgid "sample-multiserver.yml"
msgstr ""

#: ../deployment/plone_playbook.rst:101
msgid ""
"A configuration that demonstrates how to run multiple Zope/Plone installs "
"with different versions and virtual hosting."
msgstr ""

#: ../deployment/plone_playbook.rst:103
msgid ""
"The first four samples are meant to be immediately useful. Just copy and "
"customize. The multiserver sample is just a demonstration of several "
"customization techniques. Read it for examples, but don't expect to use it "
"without substantial customization."
msgstr ""

#: ../deployment/plone_playbook.rst:108
msgid ""
"Why no ``sample-large.yml``? Because a larger server installation is always "
"going to require more thought and customization. We'll discuss those "
"customization points later. The ``sample-medium.yml`` file will give you a "
"starting point."
msgstr ""

#: ../deployment/plone_playbook.rst:114
msgid "Tests"
msgstr ""

#: ../deployment/plone_playbook.rst:116
msgid ""
"You'll find a :file:`tests.py` program file and a :file:`tests` directory. "
"The :file:`tests` directory contains Doctest files to test our sample "
"configurations. You may add your own."
msgstr ""

#: ../deployment/plone_playbook.rst:120
msgid ""
"The :file:`tests.py` program is a convenience script that will run one or "
"more of the Vagrant boxes against one or more of the Doctest files. Run it "
"with no command line argument for usage help. Or, read the source ;)"
msgstr ""

#: ../deployment/plone_stack.rst:3
msgid "Intro to Plone Stack"
msgstr ""

#: ../deployment/plone_stack.rst:5
msgid ""
"If you haven't read the first couple of chapters of `Guide to deploying and "
"installing Plone in production "
"<http://docs.plone.org/manage/deploying/index.html>`_, take a moment to do "
"so. You'll want to be familiar with the main components of a typical Plone "
"install for deployment and know when each is vital and when unnecessary."
msgstr ""

#: ../deployment/plone_stack.rst:10
msgid ""
"The generic components of a full-stack Plone installation. Not all are "
"always used."
msgstr ""

#: ../deployment/plone_stack.rst:12
msgid "The Plone Ansible Playbook makes choices for each generic component."
msgstr ""

#: ../deployment/plone_stack.rst:18
msgid "The specific components used in Plone's Ansible Playbook."
msgstr ""

#: ../deployment/plone_stack.rst:20
msgid ""
"You are not stuck with our choices. If, for example, you wish to use Apache "
"rather than Nginx for the web server component, that won't be a particular "
"problem. You'll just need to do more work to customize."
msgstr ""
